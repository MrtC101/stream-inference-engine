{"config":{"lang":["en","es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"1_overview/","title":"Overview draft","text":""},{"location":"1_overview/#stream-inference-engine","title":"Stream Inference Engine","text":"<p>This project implements a real-time video inference system designed to process multiple concurrent RTSP streams under strict latency and resource constraints.</p> <p>The system targets continuous operation (24/7) on a single GPU host, prioritizing predictable degradation over maximum throughput. The primary design goal is not peak performance, but controlled behavior under load: bounded latency, explicit backpressure, and failure isolation between streams.</p> <p>This documentation describes the system from an engineering perspective, focusing on architectural boundaries, performance characteristics, and design trade-offs rather than implementation details.</p> <p>This project presents a technical overview of a MindColab's new real-time video processing engine designed to use interchangeable AI models to perform inference over multiple continuous video streams. The system, hereafter referred to as inference engine, acts as a core piece for a microservice architecture used to deliver a commercial service called \"ironstream\". To make the engine a reliable solution, the implementation and internal design need to meet key requirements such as 24/7 availability, efficient use of computational resources, state-of-the-art frameworks and extensibility and customization of inference features.</p> <p>The primary goal is not to showcase a finished product, but to document engineering decisions, trade-offs, and real-world limitations encountered while building a functional MVP under practical constraints such as time, hardware, and system complexity.</p> <p>The system evolved from an initial CPU-only implementation\u2014unable to sustain real-time performance\u2014into a GPU-accelerated architecture based on GStreamer, CUDA/NVMM, and decoupled inference components. </p> <p>The first CPU-bound approach took over 3 months of implementation to reach a functional starting point. This implementation included - Project setup using uv  - Development container with cpu-bound libraries  - Configuration yaml field validation using Cerverus - Design of a DSL grammar for Inference Rules and implementation of a interpreter built with lark - DSL to create yaml configurations and templates to inference specifications and streams configuration. - OpenCV based approach to read from stream sources. - A zero-copy Frame transmition using numpy. - Motion detection mechanism to trigger inference over specific frames.</p> <p>The second step was to implement a GPU-bound apporach over the next 4 months.  - Re-implementation of zero-copy approach using pytorch tensors. - Re design  Development container to install GPU specifi frameworks as Cuda, CDNN, Deepstream, Gstreamer - First intent of a production ready docker container - Inference model architecture to add Inference models extensibiliy - Implementing a multiprocessing approch with custom IPC and shared memory approach to transfer frames between piplines using gstreamr's shmsrc. - Implementing a numba kernel to draw lines in a GPU - Integratting drawing module to show visual metada and text using deepstream plugin nvdsosd - Implement and optimize dockerfile and docker compose file to build the lightests posible docker image - Developed simple API to query stream processing jobs called \"runs\" and store the in a sqlite database. - Implemented mechanism to read database runs, update state - Implemente engine metric capture mechanism to evaluate performance . notebook with analisis of system over stress with the inference engien in mvp state.</p>"},{"location":"1_overview/#project-scope","title":"Project Scope","text":"<p>The current scope of the project includes:</p> <ul> <li>Real-time video stream ingestion.</li> <li>GPU-accelerated processing and inference.</li> <li>Re-streaming of processed video with bounded latency.</li> <li>Continuous operation in production-like environments (24/7).</li> <li>Hot-reload support for selected configuration parameters.</li> </ul> <p>Explicitly out of scope at this stage:</p> <ul> <li>Automatic horizontal scaling.</li> <li>Advanced orchestration (Kubernetes, autoscaling).</li> <li>Fully guaranteed end-to-end zero-copy.</li> <li>Dynamic redeploy without process restart.</li> <li>Exhaustive performance analysis and deep profiling.</li> </ul>"},{"location":"1_overview/#design-principles","title":"Design Principles","text":"<p>Engineering decisions throughout the project prioritized:</p> <ul> <li>Operational simplicity over maximum flexibility.</li> <li>Clear separation of responsibilities between pipelines.</li> <li>Minimal yet sufficient observability for an MVP.</li> <li>Incremental evolution instead of premature over-engineering.</li> <li>Explicit use of NVMM buffers to reduce unnecessary memory copies.</li> </ul> <p>The system is designed to be understandable, modifiable, and extensible, even where optimizations are intentionally left unimplemented.</p>"},{"location":"1_overview/#current-state-mvp","title":"Current State (MVP)","text":"<p>In its current state, the system:</p> <ul> <li>Operates in real time under controlled conditions.</li> <li>Measures end-to-end latency in independent pipeline segments.</li> <li>Uses GPU resources consistently, with basic metrics for GPU, VRAM, and RAM utilization.</li> <li>Supports controlled restarts, graceful shutdown, and partial configuration hot-reload.</li> <li>Contains known inefficiencies, particularly in rendering and frame drawing stages.</li> </ul> <p>This state is considered intentionally incomplete, but stable and observable.</p>"},{"location":"1_overview/#target-audience","title":"Target Audience","text":"<p>This documentation is intended for:</p> <ul> <li>Engineers interested in real-time video architectures.</li> <li>Technical reviewers evaluating engineering judgment, not just features.</li> <li>Teams that value explicit acknowledgment of limitations over polished claims.</li> </ul> <p>It is not meant to be end-user documentation or marketing material.</p>"},{"location":"1_overview/#how-to-read-this-documentation","title":"How to Read This Documentation","text":"<p>The documentation is structured progressively:</p> <ol> <li>Overall system architecture.</li> <li>Performance and latency considerations.</li> <li>Key engineering decisions.</li> <li>Current limitations and technical debt.</li> <li>Production and operational considerations.</li> </ol> <p>Each section can be read independently, but the full value emerges from understanding the system as a whole.</p>"},{"location":"2_architecture/","title":"Architecture","text":""},{"location":"2_architecture/#modulos-conceptuales","title":"M\u00f3dulos conceptuales","text":"<ul> <li>API: Comunicaci\u00f3n con desarrolladores y otros servicios, validaci\u00f3n b\u00e1sica de configuraciones y pesos, administraci\u00f3n de base de datos.</li> <li>Engine: Orquestador de subm\u00f3dulos, interact\u00faa con la base de datos mediante polling.</li> <li>Configuration Validation: Interpreta argumentos del YAML, enlaza templates, contiene el compilador DSL de reglas de inferencia, genera clases que ejecutan reglas por frame.</li> <li>Metrics: Monitorea recursos del sistema y reporta resultados a la base de datos.</li> <li>Processing: Orquesta detecci\u00f3n de movimiento, inferencia de modelos, dibujado de frames y ejecuci\u00f3n de reglas.</li> <li>Evaluador de reglas: Ejecuta reglas a partir de los resultados de inferencia (orquestado por Processing).</li> <li>M\u00f3dulo de dibujado: Dibuja y genera metadatos sobre frames seg\u00fan resultados de inferencia y evaluaci\u00f3n de reglas (orquestado por Processing).</li> <li>M\u00f3dulo de modelos: Gestiona carga de pesos y arquitecturas de modelos, valida configuraciones y clases a detectar o segmentar.</li> <li>Stream y File Manager: Ejecuta procesos de inferencia sobre streams o archivos, crea pipelines de GPU/CPU, levanta el servidor RTSP, sirve al Engine.</li> <li>Transforms: Puente entre Processing y Stream/File Manager, permite el paso de frames entre pipelines.</li> <li>Flujo de datos conceptual: Los frames y resultados de inferencia fluyen del Stream/File Manager \u2192 Transform \u2192 Processing (incluyendo dibujado y reglas) \u2192 Transform \u2192 Stream/File Manager \u2192 Server RTSP. La API y la base de datos act\u00faan como mediadores de control y configuraci\u00f3n.</li> </ul> <p>[Aqu\u00ed ir\u00eda el diagrama conceptual del flujo de datos y control entre los m\u00f3dulos]</p>"},{"location":"2_architecture/#flujo-de-control-y-comunicacion","title":"Flujo de control y comunicaci\u00f3n","text":"<ul> <li>Los m\u00f3dulos se comunican mediante llamadas a m\u00e9todos conceptuales entre ellos.</li> <li>Transform utiliza IPC y memoria compartida para coordinar pipelines que corren en procesos separados.</li> <li>API y Engine se comunican exclusivamente a trav\u00e9s de la base de datos.</li> <li>Todos los m\u00f3dulos deben funcionar correctamente para ejecutar un run; si alguno falla, el run no se completa.</li> </ul>"},{"location":"2_architecture/#dependencias-externas-y-hardware","title":"Dependencias externas y hardware","text":"<ul> <li>Dependencia fuerte de arquitectura GPU: Tesla V100, Google T4, Jetson.</li> <li>Librer\u00edas y frameworks requeridos: CUDA 12.6, cuDNN, DeepStream 7.1, pyds, GStreamer plugins, drivers NVIDIA espec\u00edficos, YOLO, PyTorch, TensorRT.</li> <li>API y Engine dependen de la base de datos (actualmente local).</li> <li>M\u00f3dulos dependen del hardware disponible para cumplir con rendimiento esperado.</li> </ul>"},{"location":"2_architecture/#escalabilidad-y-limites-conceptuales","title":"Escalabilidad y l\u00edmites conceptuales","text":"<ul> <li>L\u00edmite actual: 6 streams 1080p con inferencia full-frame usando modelo YOLO nano.</li> <li>Potencial estimado: 12-15 streams con optimizaci\u00f3n de recursos y sin escalar hardware.</li> <li>Consumo actual para 6 streams: 16GB RAM, &lt;12GB VRAM, GPU Tesla V100, CPU 6 n\u00facleos.</li> <li>Escalar mediante m\u00e1s hardware es m\u00e1s r\u00e1pido pero costoso; optimizaci\u00f3n de c\u00f3digo puede mejorar eficiencia sin aumentar recursos.</li> </ul>"},{"location":"2_architecture/#metricas-conceptuales","title":"M\u00e9tricas conceptuales","text":"<ul> <li>Se mide framedrop rate, latencia end-to-end y latencia de procesamiento.</li> <li>Pipeline 2 mide solo latencia end-to-end, framedrop no es representativo.</li> <li>La medici\u00f3n real es limitada debido a la p\u00e9rdida de identidad de frames entre pipelines.</li> <li>Futuras mejoras: agregar metadata por frame para trazabilidad entre pipelines y medici\u00f3n precisa de throughput y latencia.</li> </ul>"},{"location":"2_architecture/#extensibilidad-y-ownership","title":"Extensibilidad y ownership","text":"<ul> <li>M\u00f3dulos como validaci\u00f3n de reglas, evaluaci\u00f3n de reglas, carga de modelos y dibujado son extensibles, aunque requieren intervenci\u00f3n de un desarrollador.</li> <li>Engine orquesta todos los m\u00f3dulos; API desacopla casi totalmente la interacci\u00f3n mediante la base de datos.</li> <li>Dise\u00f1o conceptual permite escalar y modificar funcionalidades sin afectar otros m\u00f3dulos, respetando l\u00edmites del MVP.</li> </ul>"},{"location":"3_performance/","title":"Performance draft","text":"<ol> <li>performance.md \u2014 Performance &amp; Measurements (clave)</li> </ol> <p>Esto es lo que m\u00e1s pesa para perfiles como el tuyo.</p> <p>Contenido:</p> <p>Throughput</p> <p>Streams, resolution, FPS</p> <p>Latency</p> <p>Explicar que es pipeline dividido</p> <p>D\u00f3nde se mide y por qu\u00e9</p> <p>Resource utilization</p> <p>GPU %</p> <p>VRAM</p> <p>CPU / RAM (si suma)</p> <p>What was improved</p> <p>Before: CPU-bound, not real-time</p> <p>After: real-time with quality trade-offs</p> <p>No pongas gr\u00e1ficos si no aportan. N\u00fameros claros &gt; gr\u00e1ficos lindos.</p>"},{"location":"4_engineering-decisions/","title":"Engineering Decisions draft","text":"<p>Ac\u00e1 se ve madurez.</p> <p>Ejemplos de secciones:</p> <p>Why GStreamer + DeepStream</p> <p>Why GPU-first design</p> <p>Why partial zero-copy</p> <p>Why Python (and where it hurts)</p> <p>En cada una:</p> <p>decisi\u00f3n</p> <p>beneficio</p> <p>costo / limitaci\u00f3n</p> <p>Eso es pensamiento de ingeniero, no de junior.</p>"},{"location":"5_limitations/","title":"Limitations draft","text":"<p>Esto no te debilita, te fortalece.</p> <p>Contenido:</p> <p>No full end-to-end zero-copy</p> <p>No horizontal scaling</p> <p>Limited hot-reload (no source swap)</p> <p>No advanced fault isolation yet</p> <p>Corto, honesto, t\u00e9cnico.</p>"},{"location":"6_production/","title":"Production draft","text":"<p>Solo si es real.</p> <p>24/7 operation assumptions</p> <p>Restart strategy</p> <p>Graceful shutdown</p> <p>Configuration hot-reload via DSL</p> <p>Nada de promesas futuras.</p>"},{"location":"","title":"Stream Inference Engine","text":""},{"location":"#problem-to-solve","title":"Problem to solve","text":"<p>This project implements a real-time video inference system designed to process multiple concurrent RTSP streams under strict input, output, performance and resource constraints. </p>"},{"location":"#constraints","title":"Constraints","text":""},{"location":"#system-input","title":"System Input","text":"<ul> <li>All Input streams are from different remote sources.</li> <li>All sources are assumed to stream using RTSP protocol over http/s and are h265 encoded.</li> <li>Remote connection is unstable and can be lost at any time.</li> </ul>"},{"location":"#system-output","title":"System Output","text":"<ul> <li>streams have to be served using RTSP over http and coded in H264 or H256 codec.  </li> <li>Inference results are shown as layouts that are drawn over the frame</li> <li>No metadata can be generated to be transmitted with the stream.</li> <li>Inference should trigger events based on the inference rules declared in a DSL YAML configuration file.  These events should be consumed externally in the future.</li> <li></li> </ul>"},{"location":"#system-output_1","title":"System Output","text":"<ul> <li>Output should be coded in a H264 format stream served by an RTSP server </li> <li>Achieve real-time output keeping frame drop rate metric under 5% </li> <li>Inference result overlays must be plotted on top of the input video</li> <li>No ovelayes medata is generated or transfered as output</li> </ul>"},{"location":"#inference-contraints","title":"Inference Contraints","text":"<ul> <li>System must be able to process a DSL inference configuration YAML file.</li> <li>System must be able to hot-reload inference configurations while changed.</li> <li>System must provide a templates and configurations processing capability to ensure reusability. </li> <li>Capability to start and stop inference processing over each different processed stream.</li> </ul>"},{"location":"#resources-constraints","title":"Resources constraints","text":"<ul> <li>Reduce CPU usage</li> <li>Do stream decode and encode using hardware</li> <li>Do inference using GPU computation  </li> </ul>"},{"location":"#responsibility-boundaries","title":"Responsibility boundaries","text":"<ul> <li>System must reconnect autocamitaclly if input stream is lost.</li> <li>System must be able to handle multiple concurrent streams.</li> <li>System must provide an API to load configuration files, model weights, and run inference streams over input streams.</li> </ul>"},{"location":"es/1_overview/","title":"Resumen","text":"<p>Esta secci\u00f3n describe la naturaleza del Stream Inference Engine, su posici\u00f3n dentro de la arquitectura global y las responsabilidades que asume en el procesamiento de streams. Define los l\u00edmites funcionales del sistema, su modelo de integraci\u00f3n con otros servicios, la gesti\u00f3n de configuraciones y estado, as\u00ed como el comportamiento ante errores. Finalmente, contextualiza el estado actual como MVP y las l\u00edneas de evoluci\u00f3n orientadas a escalabilidad y mejora de observabilidad.</p>"},{"location":"es/1_overview/#naturaleza-del-sistema","title":"Naturaleza del sistema","text":"<ul> <li>El sistema funciona como un motor de inferencia que ejecuta runtimes de procesamiento de video.</li> <li>Ejecuta exclusivamente reglas definidas por el negocio.</li> <li>No toma decisiones aut\u00f3nomas ni aplica l\u00f3gica de negocio propia.</li> <li>Brinda flexibilidad para inferencia personalizada por cliente.</li> </ul>"},{"location":"es/1_overview/#rol-dentro-de-la-arquitectura","title":"Rol dentro de la arquitectura","text":"<ul> <li>Es parte de una arquitectura de microservicios m\u00e1s amplia.</li> <li>Implementa tres m\u00f3dulos l\u00f3gicos principales:</li> <li>M\u00f3dulo de inferencia</li> <li>M\u00f3dulo de configuraciones y modelos</li> <li>M\u00f3dulo de dibujado</li> <li>Estos m\u00f3dulos se integran en un orquestador denominado <code>inference_engine</code>.</li> <li>El engine obtiene los trabajos a ejecutar mediante polling en una base de datos.</li> <li>La API act\u00faa como intermediario entre otros servicios y el engine.</li> </ul>"},{"location":"es/1_overview/#responsabilidades-principales","title":"Responsabilidades principales","text":"<ul> <li>Ejecutar inferencia sobre streams de video RTSP.</li> <li>Aplicar reglas previamente definidas sobre los resultados de inferencia.</li> <li>Dibujar los resultados directamente sobre los frames.</li> <li>Publicar el stream resultante mediante RTSP.</li> <li>Mantener la ejecuci\u00f3n de los streams independientemente de la existencia de consumidores.</li> </ul>"},{"location":"es/1_overview/#gestion-de-configuraciones-y-modelos","title":"Gesti\u00f3n de configuraciones y modelos","text":"<ul> <li>Las configuraciones y pesos de modelos son administrados por la API.</li> <li>El engine consume \u00fanicamente configuraciones y recursos previamente cargados.</li> <li>Permite inferencia personalizada a partir de configuraciones espec\u00edficas por cliente.</li> </ul>"},{"location":"es/1_overview/#manejo-de-estado-y-persistencia","title":"Manejo de estado y persistencia","text":"<ul> <li>Persiste estado de corto plazo necesario para ejecutar reglas entre frames.</li> <li>El estado se utiliza para detectar eventos dependientes de historial.</li> <li>Los eventos pueden derivar en dibujado visual o notificaci\u00f3n futura.</li> <li>No se persiste informaci\u00f3n hist\u00f3rica de inferencia a largo plazo.</li> </ul>"},{"location":"es/1_overview/#exposicion-e-integracion","title":"Exposici\u00f3n e integraci\u00f3n","text":"<ul> <li>La autenticaci\u00f3n y autorizaci\u00f3n est\u00e1n fuera de scope.</li> <li>El acceso est\u00e1 delegado a otras capas de la arquitectura.</li> <li>El sistema expone streams RTSP sin asumir la existencia de consumidores activos.</li> <li>Otros servicios pueden consumir streams o eventos de forma independiente.</li> </ul>"},{"location":"es/1_overview/#manejo-de-errores-y-ejecucion-de-runs","title":"Manejo de errores y ejecuci\u00f3n de runs","text":"<ul> <li>Si una fuente no es alcanzable inicialmente, el run se marca como fallido.</li> <li>Ante p\u00e9rdida de conexi\u00f3n, se aplican reintentos configurables.</li> <li>Si los reintentos fallan, el run se marca como fallido.</li> <li>Los errores de implementaci\u00f3n se consideran no recuperables.</li> <li>El estado de los runs se persiste en la base de datos.</li> </ul>"},{"location":"es/1_overview/#estado-actual-del-sistema","title":"Estado actual del sistema","text":"<ul> <li>El sistema se encuentra en estado MVP.</li> <li>El foco principal fue la validaci\u00f3n de viabilidad t\u00e9cnica y de negocio.</li> <li>Se realizaron pruebas de estr\u00e9s para identificar l\u00edmites de rendimiento.</li> <li>El sistema presenta margen de mejora con desarrollo adicional.</li> </ul>"},{"location":"es/1_overview/#evolucion-prevista","title":"Evoluci\u00f3n prevista","text":"<ul> <li>La evoluci\u00f3n prioriza escalabilidad del sistema.</li> <li>La escalabilidad impactar\u00e1 en throughput y latencia.</li> <li>La medici\u00f3n precisa de m\u00e9tricas es actualmente limitada.</li> <li>La arquitectura de dos pipelines afecta la trazabilidad de los frames. s</li> </ul>"},{"location":"es/2_architecture/","title":"Arquitectura","text":"<p>Esta secci\u00f3n describe la descomposici\u00f3n estructural del sistema en m\u00f3dulos conceptuales, el flujo de datos y control entre ellos, y los mecanismos de comunicaci\u00f3n utilizados. Define las dependencias externas cr\u00edticas (hardware, drivers y frameworks), los l\u00edmites actuales de escalabilidad bajo el hardware objetivo y las m\u00e9tricas consideradas para evaluar comportamiento. Tambi\u00e9n delimita el grado de extensibilidad de los componentes y el modelo de ownership dentro del alcance del MVP.</p>"},{"location":"es/2_architecture/#modulos-conceptuales","title":"M\u00f3dulos conceptuales","text":"<ul> <li>API: Comunicaci\u00f3n con desarrolladores y otros servicios, validaci\u00f3n b\u00e1sica de configuraciones y pesos, administraci\u00f3n de base de datos.</li> <li>Engine: Orquestador de subm\u00f3dulos, interact\u00faa con la base de datos mediante polling.</li> <li>Configuration Validation: Interpreta argumentos del YAML, enlaza templates, contiene el compilador DSL de reglas de inferencia, genera clases que ejecutan reglas por frame.</li> <li>Metrics: Monitorea recursos del sistema y reporta resultados a la base de datos.</li> <li>Processing: Orquesta detecci\u00f3n de movimiento, inferencia de modelos, dibujado de frames y ejecuci\u00f3n de reglas.</li> <li>Evaluador de reglas: Ejecuta reglas a partir de los resultados de inferencia (orquestado por Processing).</li> <li>M\u00f3dulo de dibujado: Dibuja y genera metadatos sobre frames seg\u00fan resultados de inferencia y evaluaci\u00f3n de reglas (orquestado por Processing).</li> <li>M\u00f3dulo de modelos: Gestiona carga de pesos y arquitecturas de modelos, valida configuraciones y clases a detectar o segmentar.</li> <li>Stream y File Manager: Ejecuta procesos de inferencia sobre streams o archivos, crea pipelines de GPU/CPU, levanta el servidor RTSP, sirve al Engine.</li> <li>Transforms: Puente entre Processing y Stream/File Manager, permite el paso de frames entre pipelines.</li> <li>Flujo de datos conceptual: Los frames y resultados de inferencia fluyen del Stream/File Manager \u2192 Transform \u2192 Processing (incluyendo dibujado y reglas) \u2192 Transform \u2192 Stream/File Manager \u2192 Server RTSP. La API y la base de datos act\u00faan como mediadores de control y configuraci\u00f3n.</li> </ul> <pre><code>graph TB\n    subgraph API[\"API\"]\n        API_Handlers[Handlers]\n        API_Services[Services]\n        API_Data[Data Access]\n    end\n\n    Database[(SQLite)]\n\n    subgraph StreamEngine[\"Stream Inference Engine\"]\n        subgraph Lifecycle[\"Run Lifecycle Module\"]\n            Run_Manager[\"Run Manager (Starter/Stopper)\"]\n            Run_Metrics[Run Metrics Collector]\n        end\n\n        subgraph Managers[\"Stream Managers\"]\n            StreamMgr[Stream Manager]\n            FileMgr[File Manager]\n        end\n\n        subgraph Pipelines[\"GStreamer Pipelines\"]\n            subgraph Pipeline_File[\"Pipeline - File\"]\n                CPUGPUPath_File[CPU/GPU Path]\n                subgraph ConfigMgr_File[\"Configuration Manager\"]\n                    DSL_File[DSL Validator]\n                end\n            end\n            subgraph Pipeline_W1[\"Pipeline - Worker 1\"]\n                CPUGPUPath_W1[CPU/GPU Path]\n                subgraph ConfigMgr_W1[\"Configuration Manager\"]\n                    DSL_W1[DSL Validator]\n                end\n            end\n            subgraph Pipeline_WN[\"Pipeline - Worker N\"]\n                CPUGPUPath_WN[CPU/GPU Path]\n                subgraph ConfigMgr_WN[\"Configuration Manager\"]\n                    DSL_WN[DSL Validator]\n                end\n            end\n        end\n\n        subgraph Processors[\"Frame Processors\"]\n            subgraph FP_File[\"FrameProcessor - File\"]\n                FP_File_Draw[Drawing Module]\n                FP_File_Model[Models Module]\n                FP_File_Rules[Rules Module]\n                FP_File_Motion[Motion Detection Module]\n            end\n\n            subgraph FP_W1[\"FrameProcessor - Worker 1\"]\n                FP_W1_Draw[Drawing Module]\n                FP_W1_Model[Models Module]\n                FP_W1_Rules[Rules Module]\n                FP_W1_Motion[Motion Detection Module]\n            end\n\n            subgraph FP_WN[\"FrameProcessor - Worker N\"]\n                FP_WN_Draw[Drawing Module]\n                FP_WN_Model[Models Module]\n                FP_WN_Rules[Rules Module]\n                FP_WN_Motion[Motion Detection Module]\n            end\n        end\n    end\n\n    %% Conexiones API con exterior\n    External[External Clients] --&gt; API_Handlers\n\n    %% Flujo interno API (3 niveles)\n    API_Handlers --&gt; API_Services\n    API_Services --&gt; API_Data\n\n    %% Run Lifecycle delega a Stream Managers\n    Run_Manager --&gt; StreamMgr\n    Run_Manager --&gt; FileMgr\n\n    %% Stream Managers lanzan Pipelines\n    StreamMgr --&gt; Pipeline_W1 &amp; Pipeline_WN\n\n    %% File Manager lanza Pipeline\n    FileMgr --&gt; Pipeline_File\n\n    %% CPU/GPU Path conecta a FrameProcessors\n    CPUGPUPath_File --&gt; FP_File\n    CPUGPUPath_W1 --&gt; FP_W1\n    CPUGPUPath_WN --&gt; FP_WN\n\n    %% Configuration Manager provee servicio a Frame Processors\n    ConfigMgr_File -.-&gt;|Service| FP_File\n    ConfigMgr_W1 -.-&gt;|Service| FP_W1\n    ConfigMgr_WN -.-&gt;|Service| FP_WN\n\n    %% Run Metrics Collector monitorea Frame Processors\n    Run_Metrics -.-&gt; Processors\n\n    %% Conexiones con la Base de Datos\n    API_Data &lt;--&gt;|Read/Write| Database\n    Database &lt;--&gt;|Read/Write| Lifecycle\n\n    style Database fill:#f9f,stroke:#333,stroke-width:3px\n    style API fill:#e1f5ff,stroke:#01579b,stroke-width:2px\n    style StreamEngine fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    style Lifecycle fill:#fff9c4,stroke:#f57f17,stroke-width:1px\n    style Managers fill:#e8f5e9,stroke:#2e7d32,stroke-width:1px\n    style Pipelines fill:#f3e5f5,stroke:#6a1b9a,stroke-width:1px\n    style Processors fill:#fce4ec,stroke:#c2185b,stroke-width:1px</code></pre>"},{"location":"es/2_architecture/#flujo-de-control-y-comunicacion","title":"Flujo de control y comunicaci\u00f3n","text":"<ul> <li>Los m\u00f3dulos se comunican mediante llamadas a m\u00e9todos conceptuales entre ellos.</li> <li>Stream Manager utiliza IPC (stdin/stdout) y memoria compartida (shmsink/shmsrc) para coordinar pipelines que corren en procesos separados. Los Transforms procesan frames directamente dentro del mismo proceso mediante llamadas al FrameProcessor.</li> <li>API y Engine se comunican exclusivamente a trav\u00e9s de la base de datos.</li> <li>Todos los m\u00f3dulos deben funcionar correctamente para ejecutar un run; si alguno falla, el run no se completa.</li> </ul>"},{"location":"es/2_architecture/#dependencias-externas-y-hardware","title":"Dependencias externas y hardware","text":"<ul> <li>Dependencia fuerte de arquitectura GPU: Tesla V100, NVIDIA T4, Jetson.</li> <li>Librer\u00edas y frameworks requeridos: CUDA 12.6, cuDNN, DeepStream 7.1, pyds, GStreamer plugins, drivers NVIDIA espec\u00edficos, YOLO, PyTorch, TensorRT.</li> <li>API y Engine dependen de la base de datos (actualmente local).</li> <li>M\u00f3dulos dependen del hardware disponible para cumplir con rendimiento esperado.</li> </ul>"},{"location":"es/2_architecture/#escalabilidad-y-limites-conceptuales","title":"Escalabilidad y l\u00edmites conceptuales","text":"<ul> <li>L\u00edmite actual: 6 streams 1080p con inferencia full-frame usando modelo YOLO nano.</li> <li>Potencial estimado: ~10 streams con optimizaci\u00f3n de recursos y sin escalar hardware.</li> <li>Consumo actual para 6 streams: 16GB RAM, &lt;12GB VRAM, GPU Tesla V100, CPU 6 n\u00facleos.</li> <li>Escalar mediante m\u00e1s hardware es m\u00e1s r\u00e1pido pero costoso; optimizaci\u00f3n de c\u00f3digo puede mejorar eficiencia sin aumentar recursos.</li> </ul>"},{"location":"es/2_architecture/#extensibilidad-y-ownershipa","title":"Extensibilidad y ownershipa","text":"<ul> <li>M\u00f3dulos como validaci\u00f3n de reglas, evaluaci\u00f3n de reglas, carga de modelos y dibujado son extensibles, aunque requieren intervenci\u00f3n de un desarrollador.</li> <li>Engine orquesta todos los m\u00f3dulos; API desacopla casi totalmente la interacci\u00f3n mediante la base de datos.</li> <li>Dise\u00f1o conceptual permite escalar y modificar funcionalidades sin afectar otros m\u00f3dulos, respetando l\u00edmites del MVP.</li> </ul>"},{"location":"es/3_performance/","title":"Rendimiento","text":"<p>Esta secci\u00f3n describe el modelo de evaluaci\u00f3n de rendimiento del sistema en el contexto del MVP. Define el entorno de validez, los criterios formales de \u201creal-time\u201d, las m\u00e9tricas utilizadas y sus limitaciones. No constituye un benchmark comparativo ni expone capacidad m\u00e1xima del sistema.</p>"},{"location":"es/3_performance/#alcance-y-contexto-de-validez","title":"Alcance y contexto de validez","text":"<p>Las consideraciones de este documento son v\u00e1lidas \u00fanicamente bajo un entorno controlado equivalente al utilizado durante el desarrollo del MVP:</p> <ul> <li>Arquitectura x86_64  </li> <li>GPU NVIDIA clase datacenter  </li> <li>CPU multin\u00facleo  </li> <li>CUDA + TensorRT + DeepStream compatibles  </li> <li>Streams homog\u00e9neos  </li> <li>Resoluciones entre 480p y 1080p  </li> <li>FPS de entrada en rango 25\u201330  </li> </ul> <p>Cualquier variaci\u00f3n en hardware, drivers, modelos o configuraci\u00f3n invalida la extrapolaci\u00f3n directa del comportamiento descrito.</p>"},{"location":"es/3_performance/#definicion-de-carga","title":"Definici\u00f3n de carga","text":"<p>Se entiende por carga la cantidad de streams procesados en paralelo, cada uno asociado a uno o m\u00e1s modelos de inferencia.</p> <p>El incremento de carga impacta en:</p> <ul> <li>FPS efectivo por stream  </li> <li>Frame drop rate agregado  </li> <li>Latencias promedio  </li> <li>Utilizaci\u00f3n de GPU y memoria  </li> </ul> <p>El sistema presenta degradaci\u00f3n progresiva cuando la contenci\u00f3n de recursos impide sostener el procesamiento en tiempo real para todos los streams activos.</p>"},{"location":"es/3_performance/#criterios-de-aceptacion-real-time","title":"Criterios de aceptaci\u00f3n (Real-Time)","text":"<p>El estado del sistema se clasifica seg\u00fan el frame drop rate agregado:</p> <ul> <li>0% \u2013 1%   Tiempo real \u00f3ptimo dentro del alcance del MVP.</li> <li>1% \u2013 5%   Rendimiento aceptable para validaci\u00f3n funcional.</li> <li>&gt; 5%   El servicio deja de considerarse real-time en el contexto del MVP.</li> </ul> <p>Estos umbrales est\u00e1n definidos con fines de validaci\u00f3n t\u00e9cnica interna y no representan SLA productivos.</p> <p>El estado real-time del sistema se determina utilizando el valor agregado correspondiente a <code>workers_frame_drop_rate_avg</code>.</p>"},{"location":"es/3_performance/#metodologia-de-medicion-alto-nivel","title":"Metodolog\u00eda de medici\u00f3n (alto nivel)","text":"<ul> <li>Las m\u00e9tricas excluyen warm-up y creaci\u00f3n de pipelines.</li> <li>Se capturan \u00fanicamente en r\u00e9gimen estable.</li> <li>Se asume homogeneidad entre streams.</li> <li>Las m\u00e9tricas por stream se promedian temporalmente.</li> <li>Las m\u00e9tricas del sistema son agregaciones peri\u00f3dicas de todas las m\u00e9tricas por stream.</li> <li>No se realiza detecci\u00f3n ni eliminaci\u00f3n de outliers.</li> </ul> <p>El objetivo es caracterizar comportamiento general bajo carga, no realizar an\u00e1lisis estad\u00edstico exhaustivo.</p>"},{"location":"es/3_performance/#metricas-utilizadas","title":"M\u00e9tricas utilizadas","text":"<p>Las m\u00e9tricas utilizadas son agregados persistidos por el Engine y derivados del modelo <code>system_metrics</code>.</p> <p>Se consideran:</p>"},{"location":"es/3_performance/#worker-pipelines","title":"Worker pipelines","text":"<ul> <li><code>workers_fps_avg</code></li> <li><code>workers_frame_drop_rate_avg</code></li> <li><code>workers_processing_latency_avg_ms</code></li> <li><code>workers_e2e_latency_avg_ms</code></li> </ul>"},{"location":"es/3_performance/#factory-pipelines","title":"Factory pipelines","text":"<ul> <li><code>factories_fps_avg</code></li> <li><code>factories_e2e_latency_avg_ms</code></li> </ul>"},{"location":"es/3_performance/#recursos-del-sistema","title":"Recursos del sistema","text":"<ul> <li><code>gpu_compute_percent</code></li> <li><code>gpu_memory_percent</code></li> <li><code>ram_percent</code></li> <li><code>cpu_percent</code></li> </ul> <p>El frame drop rate es la m\u00e9trica principal de evaluaci\u00f3n, ya que representa de forma directa la incapacidad del sistema de sostener el flujo de frames en tiempo real.</p>"},{"location":"es/3_performance/#comportamiento-bajo-incremento-de-carga","title":"Comportamiento bajo incremento de carga","text":"<p>A medida que aumenta la cantidad de streams en paralelo:</p> <ul> <li>El uso de GPU tiende a crecer hasta aproximarse al punto de saturaci\u00f3n.</li> <li>El frame drop rate aumenta de forma no lineal.</li> <li>La latencia end-to-end promedio se incrementa por contenci\u00f3n de recursos.</li> <li>El FPS efectivo por stream puede degradarse progresivamente.</li> </ul> <p>El punto de degradaci\u00f3n no es una propiedad te\u00f3rica del hardware, sino una consecuencia del estado actual de la implementaci\u00f3n.</p> <p>No se publica en este documento el l\u00edmite cuantitativo de streams sostenibles.</p>"},{"location":"es/3_performance/#limitaciones-de-medicion","title":"Limitaciones de medici\u00f3n","text":"<ul> <li>No se mide jitter.</li> <li>No se mide fairness entre streams.</li> <li>No se calcula latencia end-to-end real con trazabilidad de frame completa.</li> <li>No se eval\u00faa precisi\u00f3n ni calidad de inferencia.</li> <li>No se realiza an\u00e1lisis estad\u00edstico profundo.</li> </ul> <p>La p\u00e9rdida de identidad de frame entre pipelines impide el c\u00e1lculo exacto de latencia end-to-end real.</p>"},{"location":"es/3_performance/#implicancias-y-evolucion-esperada","title":"Implicancias y evoluci\u00f3n esperada","text":"<p>El l\u00edmite operativo actual del MVP est\u00e1 determinado principalmente por eficiencia de implementaci\u00f3n y uso de recursos, no por restricciones estructurales de arquitectura.</p> <p>Mejoras en:</p> <ul> <li>Gesti\u00f3n de memoria</li> <li>Reducci\u00f3n de copias innecesarias</li> <li>Control din\u00e1mico de carga</li> <li>Instrumentaci\u00f3n de m\u00e9tricas m\u00e1s precisa  </li> </ul> <p>deber\u00edan permitir aumentar la capacidad efectiva manteniendo los umbrales definidos de real-time.</p> <p>La evoluci\u00f3n futura prioriza escalabilidad horizontal por stream y control expl\u00edcito de degradaci\u00f3n.</p>"},{"location":"es/3_performance/#fuera-de-alcance","title":"Fuera de alcance","text":"<p>Este cap\u00edtulo no cubre:</p> <ul> <li>Capacidad m\u00e1xima exacta del sistema</li> <li>Procedimientos detallados de benchmarking</li> <li>Comparativas con otras implementaciones</li> <li>SLA productivos</li> <li>Escalabilidad multinodo</li> </ul>"},{"location":"es/4_engineering-decisions/","title":"Decisiones de dise\u00f1o","text":"<p>Este cap\u00edtulo documenta las decisiones t\u00e9cnicas estructurales que definieron la arquitectura actual del sistema, incluyendo motivaciones, restricciones del entorno, alternativas descartadas y costos asumidos. Expone c\u00f3mo las prioridades del MVP \u2014estabilidad, viabilidad funcional y extensibilidad\u2014 condicionaron elecciones en separaci\u00f3n de procesos, uso de GPU, gesti\u00f3n de memoria, m\u00e9tricas y desacoplamiento entre componentes, as\u00ed como las implicancias t\u00e9cnicas que estas decisiones generan para la evoluci\u00f3n futura del engine.</p>"},{"location":"es/4_engineering-decisions/#1-objetivos-tecnicos-y-prioridades","title":"1. Objetivos t\u00e9cnicos y prioridades","text":"<ul> <li>Objetivos primarios</li> <li>Best-effort zero-copy a lo largo del pipeline.</li> <li>Alta extensibilidad en cantidad de modelos, pesos y tareas de inferencia.</li> <li>Mantener siempre una salida en tiempo real (real-time output).</li> <li>Priorizaci\u00f3n</li> <li>Se prioriz\u00f3 viabilidad funcional sobre performance absoluta.</li> <li>El sistema se concibi\u00f3 como un MVP orientado a validar factibilidad t\u00e9cnica y del modelo de negocios.</li> <li>Objetivos descartados o postergados</li> <li>Zero-copy total extremo a extremo.</li> <li>Obtenci\u00f3n de m\u00e9tricas de rendimiento completamente exactas.</li> <li>Soporte amplio de frameworks de inferencia desde el inicio.</li> <li>Alcance inicial de frameworks</li> <li>Enfoque principal en YOLO.</li> <li>Modelos espec\u00edficos como anomalib fueron expl\u00edcitamente postergados.</li> </ul>"},{"location":"es/4_engineering-decisions/#2-separacion-en-procesos-y-pipelines","title":"2. Separaci\u00f3n en procesos y pipelines","text":"<ul> <li>Motivaci\u00f3n principal</li> <li>Incompatibilidades cr\u00edticas entre CUDA, librer\u00edas de procesamiento en Python, GStreamer, PyGObject y DeepStream dentro de un mismo runtime.</li> <li>Problemas observados</li> <li>Ca\u00eddas del proceso Python sin captura de excepciones.</li> <li>Fallos al:<ul> <li>Inicializar pipelines de GStreamer.</li> <li>Acceder a GstBuffer desde transforms.</li> <li>Iniciar el debugger de Python con GStreamer activo.</li> <li>Crear m\u00faltiples pipelines en un mismo runtime.</li> <li>Usar simult\u00e1neamente encoder y decoder por hardware.</li> </ul> </li> <li>Hip\u00f3tesis t\u00e9cnica</li> <li>Librer\u00edas CUDA no reentrantes.</li> <li>Corrupci\u00f3n de estado interno de memoria al cargarlas m\u00faltiples veces en un mismo runtime.</li> <li>Decisi\u00f3n</li> <li>Separaci\u00f3n estricta en procesos independientes usando <code>subprocess</code>.</li> <li>Topolog\u00eda resultante</li> </ul> <p>Inference Engine \u251c\u2500 Stream Manager Bridge (IPC) \u251c\u2500 Stream Manager \u2502 \u251c\u2500 RTSP Server \u2502 \u251c\u2500 Pipeline 1 \u2502 \u2514\u2500 Workers (0..N) \u2514\u2500 Stream Workers \u2514\u2500 Pipeline 2 (0..N)</p> <ul> <li>Comunicaci\u00f3n basada en IPC (stdin/stdout) y memoria compartida.</li> <li>Separaci\u00f3n Pipeline 1 / Pipeline 2</li> <li>Pipeline 1: ingesti\u00f3n + servidor RTSP.</li> <li>Pipeline 2: procesamiento, inferencia y dibujado.</li> <li>Motivaci\u00f3n adicional</li> <li>Incompatibilidad entre runtime de servidor RTSP y librer\u00edas DeepStream.</li> <li>Aislamiento del runtime evita colisiones de dependencias (incluyendo versiones previas como Savant).</li> <li>Costos aceptados</li> <li>Mayor latencia.</li> <li>Mayor tiempo de inicializaci\u00f3n.</li> <li>Mayor complejidad operativa.</li> <li>Beneficio</li> <li>Estabilidad del sistema y ejecuci\u00f3n consistente.</li> </ul>"},{"location":"es/4_engineering-decisions/#3-eleccion-de-gstreamer-deepstream-y-frameworks","title":"3. Elecci\u00f3n de GStreamer, DeepStream y frameworks","text":"<ul> <li>Origen de la decisi\u00f3n</li> <li>GStreamer fue un requisito impuesto externamente.</li> <li>Implementaci\u00f3n inicial descartada</li> <li>OpenCV + NumPy en CPU: viable funcionalmente, inviable para GPU zero-copy.</li> <li>Motivaci\u00f3n del cambio</li> <li>Necesidad de consumir RTSP y generar frames directamente en GPU.</li> <li>Stack seleccionado</li> <li>GStreamer + DeepStream para ingesti\u00f3n y procesamiento GPU-native.</li> <li>TensorRT como parte del runtime NVIDIA (disponible a trav\u00e9s del SDK de DeepStream y CUDA Toolkit, no como dependencia declarada del proyecto).</li> <li>Frameworks de inferencia</li> <li>YOLO como primer framework por:</li> <li>Abundancia de modelos.</li> <li>Enfoque Python-first.</li> <li>Alta extensibilidad sin modificar el core del sistema.</li> <li>Protocolo</li> <li>RTSP impuesto por las fuentes externas (limitaci\u00f3n del dominio del problema).</li> </ul>"},{"location":"es/4_engineering-decisions/#4-gestion-de-memoria-y-frames","title":"4. Gesti\u00f3n de memoria y frames","text":"<ul> <li>Principio rector</li> <li>Minimizar copias de frames (CPU\u2194GPU y GPU\u2194GPU).</li> <li>Beneficios</li> <li>Menor consumo de RAM y VRAM.</li> <li>Mejor throughput.</li> <li>Menor frame drop rate.</li> <li>Decisi\u00f3n cr\u00edtica</li> <li>Aceptar la p\u00e9rdida de identidad del frame entre pipelines.</li> <li>Justificaci\u00f3n</li> <li>Preservar identidad requiere:</li> <li>Allocaci\u00f3n adicional de memoria compartida.</li> <li>Header de metadata persistente por frame.</li> <li>Alto esfuerzo de desarrollo no compatible con tiempos del MVP.</li> <li>Consecuencia actual</li> <li>Pipeline 1 genera frames a ~25 FPS.</li> <li>Pipeline 2 consume memoria compartida a ~45 FPS, retransmitiendo el mismo frame varias veces.</li> <li>Decisiones t\u00e9cnicas espec\u00edficas</li> <li>Uso de DeepStream para generar frames directamente en GPU.</li> <li>Uso de CuPy para capturar referencias GPU en Python.</li> <li>Uso de PyTorch por su abstracci\u00f3n de tensores desacoplada del dispositivo.</li> <li>Dibujado directo sobre frame siempre que es posible.</li> <li>Uso de metadata + <code>nvdsosd</code> para texto y overlays en GPU.</li> </ul>"},{"location":"es/4_engineering-decisions/#5-concurrencia-carga-y-frame-drop","title":"5. Concurrencia, carga y frame drop","text":"<ul> <li>L\u00edmite de streams</li> <li>No impuesto expl\u00edcitamente por c\u00f3digo.</li> <li>Determinado emp\u00edricamente por consumo de recursos.</li> <li>Frame drop</li> <li>Aceptado como m\u00e9trica principal.</li> <li>Es la m\u00e9trica m\u00e1s fiel dadas las limitaciones actuales.</li> <li>Backpressure</li> <li>Implementado mediante colas que descartan frames antes del procesamiento.</li> <li>M\u00e9tricas</li> <li>FPS de salida suficiente para inferir frame drop rate.</li> <li>Control de carga</li> <li>No implementado.</li> <li>De existir, estar\u00eda basado en consumo de recursos, no en m\u00e9tricas de rendimiento.</li> </ul>"},{"location":"es/4_engineering-decisions/#6-api-base-de-datos-y-desacoplamiento","title":"6. API, base de datos y desacoplamiento","text":"<ul> <li>Decisi\u00f3n</li> <li>Desacoplar API e Inference Engine mediante base de datos.</li> <li>Motivaci\u00f3n</li> <li>Evitar sincronizaci\u00f3n de estado entre runtimes heterog\u00e9neos.</li> <li>Permitir m\u00faltiples instancias de inference engine en el futuro.</li> <li>Estado actual</li> <li>Base de datos local.</li> <li>Soporte futuro para escalado horizontal no implementado.</li> </ul>"},{"location":"es/4_engineering-decisions/#7-observabilidad-postergado","title":"7. Observabilidad (postergado)","text":"<ul> <li>Estado</li> <li>Parcialmente abordado.</li> <li>Limitado por p\u00e9rdida de identidad de frame.</li> <li>Deuda t\u00e9cnica</li> <li>M\u00e9tricas precisas de latencia por frame.</li> <li>Trazabilidad completa del pipeline.</li> </ul>"},{"location":"es/4_engineering-decisions/#8-decisiones-de-alcance","title":"8. Decisiones de alcance","text":"<ul> <li>Excluido del MVP</li> <li>Control de carga avanzado.</li> <li>M\u00e9tricas exhaustivas.</li> <li>Gesti\u00f3n completa del lifecycle.</li> <li>Razonamiento</li> <li>No cr\u00edticas para validar viabilidad del negocio.</li> </ul>"},{"location":"es/4_engineering-decisions/#9-portabilidad-y-hardware","title":"9. Portabilidad y hardware","text":"<ul> <li>Plataformas soportadas</li> <li>x86: Tesla V100, T4.</li> <li>Jetson: JetPack 6.1.</li> <li>Objetivo</li> <li>Viabilidad tanto en edge como en datacenter.</li> <li>Resultado</li> <li>Requisito cumplido, aunque con deuda t\u00e9cnica pendiente.</li> </ul>"},{"location":"es/4_engineering-decisions/#10-evolucion-futura","title":"10. Evoluci\u00f3n futura","text":"<ul> <li>Decisiones condicionantes</li> <li>Arquitectura de pipelines desacoplados con memoria compartida.</li> <li>Componentes reemplazables</li> <li>Frameworks de inferencia.</li> <li>Modelos y pesos.</li> <li>Pendiente de revisi\u00f3n</li> <li>Optimizaci\u00f3n de performance.</li> <li>Compatibilidad de librer\u00edas.</li> <li>Observabilidad y m\u00e9tricas.</li> </ul>"},{"location":"es/5_limitations/","title":"Limitaciones","text":"<p>Esta secci\u00f3n explicita las restricciones t\u00e9cnicas observadas en el estado actual del MVP, diferenciando l\u00edmites derivados de implementaci\u00f3n, entorno y prioridades de alcance respecto de limitaciones estructurales del dise\u00f1o. Define los bordes operativos actuales del sistema en t\u00e9rminos de concurrencia, zero-copy, m\u00e9tricas, escalabilidad y observabilidad, estableciendo qu\u00e9 comportamientos son inherentes al estado presente y cu\u00e1les constituyen deuda t\u00e9cnica planificada para evoluci\u00f3n futura.</p>"},{"location":"es/5_limitations/#stream-concurrency-limit-mvp","title":"Stream Concurrency Limit (MVP)","text":"<ul> <li>El MVP soporta actualmente hasta 6 streams concurrentes con rendimiento aceptable.</li> <li>Este l\u00edmite surge de consumo agregado de recursos (RAM, VRAM, encoder/decoder por hardware y c\u00f3mputo GPU), no de una restricci\u00f3n l\u00f3gica del sistema.</li> <li>El l\u00edmite no est\u00e1 impuesto por configuraci\u00f3n ni hardcoded; es el punto a partir del cual el frame drop rate supera los umbrales aceptables de real-time.</li> <li>Con optimizaci\u00f3n t\u00e9cnica adicional, se estima que el sistema podr\u00eda escalar hasta ~10 streams sobre el mismo hardware, dependiendo de:</li> <li>resoluci\u00f3n,</li> <li>FPS de entrada,</li> <li>complejidad del modelo,</li> <li>tipo de tarea de inferencia.</li> <li>Aun con optimizaci\u00f3n, el hardware impone un techo pr\u00e1ctico que limita la escalabilidad sin aumento de recursos.</li> </ul>"},{"location":"es/5_limitations/#zero-copy-limitations","title":"Zero-Copy Limitations","text":"<ul> <li>No se ha logrado un esquema de zero-copy completo a lo largo de todo el pipeline.</li> <li>En plataformas Jetson, aunque RAM y VRAM comparten el mismo chip, el zero-copy total no es viable dentro de los l\u00edmites actuales de:</li> <li>Python,</li> <li>CUDA,</li> <li>PyTorch,</li> <li>DeepStream,</li> <li>GStreamer.</li> <li>Durante la fase de dibujado:</li> <li>ciertas operaciones (por ejemplo, segmentaci\u00f3n) resultan m\u00e1s simples de implementar en CPU usando OpenCV,</li> <li>lo que introduce copias expl\u00edcitas o impl\u00edcitas de memoria.</li> <li>Algunos modelos requieren entrada en formato <code>float32</code>, lo que implica conversiones y copias adicionales.</li> <li>En la pr\u00e1ctica, los frameworks de inferencia realizan copias internas, por lo que este comportamiento es esperado.</li> <li>Conclusi\u00f3n: se aplica un enfoque best-effort zero-copy, manteniendo los frames en GPU el mayor tiempo posible, pero aceptando copias cuando son necesarias.</li> </ul>"},{"location":"es/5_limitations/#metrics-and-observability-limitations","title":"Metrics and Observability Limitations","text":"<ul> <li>La arquitectura basada en dos pipelines desacoplados mediante memoria compartida provoca la p\u00e9rdida de identidad de frame entre pipelines.</li> <li>Esto impide medir de forma precisa:</li> <li>latencia end-to-end real,</li> <li>throughput efectivo,</li> <li>correlaci\u00f3n frame-a-frame entre entrada y salida.</li> <li>Actualmente:</li> <li>el frame drop rate se utiliza como m\u00e9trica principal para evaluar comportamiento real-time,</li> <li>otras m\u00e9tricas son aproximadas.</li> <li>Implementar trazabilidad completa requerir\u00eda:</li> <li>extender el esquema de memoria compartida con headers de metadata,</li> <li>o introducir mecanismos expl\u00edcitos de sincronizaci\u00f3n entre procesos.</li> <li>Dado el alcance del MVP, esta mejora se considera deuda t\u00e9cnica.</li> </ul>"},{"location":"es/5_limitations/#mvp-scope-limitations","title":"MVP Scope Limitations","text":"<ul> <li>El sistema se desarroll\u00f3 como MVP orientado a validaci\u00f3n t\u00e9cnica y de negocio, no como producto final.</li> <li>No se implementaron:</li> <li>mecanismos de control de carga basados en recursos,</li> <li>l\u00edmites din\u00e1micos de ejecuci\u00f3n de runs,</li> <li>escalado autom\u00e1tico.</li> <li>El n\u00famero de frameworks de inferencia soportados es limitado:</li> <li>YOLO como framework principal,</li> <li>soporte puntual para Anomalib.</li> <li>La calidad de inferencia depende exclusivamente del modelo cargado; el sistema no aplica validaci\u00f3n sem\u00e1ntica de resultados.</li> <li>El sistema no garantiza:</li> <li>accesibilidad de las fuentes,</li> <li>estabilidad de la red,</li> <li>calidad de imagen de entrada,</li> <li>comportamiento de los consumidores de los streams.</li> </ul>"},{"location":"es/5_limitations/#resolved-technical-debt","title":"Resolved Technical Debt","text":"<ul> <li>Se elimin\u00f3 la cache de frames completos en CPU del m\u00f3dulo de dibujado, reduciendo el consumo excesivo de RAM observado en versiones anteriores. El m\u00f3dulo mantiene cache de resultados de inferencia y metadata de dibujado entre frames, pero ya no almacena copias de frames en memoria CPU.</li> </ul>"},{"location":"es/5_limitations/#deferred-improvements","title":"Deferred Improvements","text":"<p>Las siguientes limitaciones se consideran abordables en fases futuras de desarrollo:</p> <ul> <li>Optimizaci\u00f3n adicional del consumo de memoria RAM y VRAM.</li> <li>Mejora del lifecycle de runs y mayor resiliencia ante fallos.</li> <li>Implementaci\u00f3n de control de carga basado en m\u00e9tricas de recursos.</li> <li>Mejora de m\u00e9tricas y trazabilidad de frames.</li> <li>Ampliaci\u00f3n del soporte de frameworks y tipos de modelos.</li> </ul>"},{"location":"es/6_production/","title":"Producci\u00f3n (MVP)","text":"<p>Esta secci\u00f3n delimita el comportamiento operativo del sistema en su estado actual bajo un entorno productivo controlado de tipo single-node. Define supuestos de ejecuci\u00f3n, restricciones de despliegue, l\u00edmites de resiliencia y ausencia de automatizaci\u00f3n avanzada, dejando expl\u00edcito qu\u00e9 garant\u00edas m\u00ednimas existen y qu\u00e9 aspectos permanecen como deuda t\u00e9cnica. No constituye un dise\u00f1o definitivo de producci\u00f3n ni establece compromisos formales de disponibilidad o SLA.</p>"},{"location":"es/6_production/#1-scope-de-produccion","title":"1. Scope de Producci\u00f3n","text":"<ul> <li>Arquitectura single-node.</li> <li>Se asume 1 GPU por host.</li> <li>No se contempla alta disponibilidad (HA) ni despliegue multi-node.</li> <li>La escalabilidad horizontal queda fuera de scope del MVP.</li> <li>El l\u00edmite pr\u00e1ctico de streams est\u00e1 condicionado tanto por consumo de recursos como por restricciones arquitect\u00f3nicas del pipeline.</li> </ul>"},{"location":"es/6_production/#2-arquitectura-de-ejecucion","title":"2. Arquitectura de Ejecuci\u00f3n","text":""},{"location":"es/6_production/#21-ciclo-de-vida-del-engine","title":"2.1 Ciclo de vida del Engine","text":"<ul> <li>El engine se ejecuta de forma persistente y no finaliza por inactividad.</li> <li>Permanece en espera indefinida de solicitudes de <code>run</code>.</li> <li>Si ninguna fuente conecta durante la inicializaci\u00f3n, el sistema no se considera desplegado funcionalmente y queda en estado de espera.</li> <li>Se asume que, en ese escenario, los <code>run</code> ser\u00e1n solicitados nuevamente de forma externa.</li> <li>Este comportamiento es susceptible de cambio en futuras iteraciones.</li> </ul>"},{"location":"es/6_production/#22-estrategia-de-reintentos","title":"2.2 Estrategia de Reintentos","text":"<ul> <li>Se utiliza una pol\u00edtica de retry fijo.</li> <li>No existe backoff exponencial.</li> <li>El sistema se considera UP aunque solo una fuente haya conectado correctamente.</li> <li>Esta l\u00f3gica constituye uno de los puntos m\u00e1s d\u00e9biles del MVP y requiere ingenier\u00eda adicional para producci\u00f3n real.</li> </ul>"},{"location":"es/6_production/#3-despliegue","title":"3. Despliegue","text":""},{"location":"es/6_production/#31-targets-soportados","title":"3.1 Targets Soportados","text":"<ul> <li><code>x86_64</code> con GPU.</li> <li><code>arm64</code> (Jetson).</li> </ul>"},{"location":"es/6_production/#32-proceso-de-build","title":"3.2 Proceso de Build","text":"<ul> <li> <p>El build de im\u00e1genes se realiza en hosts de desarrollo:</p> </li> <li> <p>Host x86 para im\u00e1genes <code>x86_64</code>.</p> </li> <li>Jetson de desarrollo para im\u00e1genes <code>arm64</code>.</li> <li>No existe un pipeline CI/CD automatizado.</li> </ul>"},{"location":"es/6_production/#33-distribucion-y-activacion","title":"3.3 Distribuci\u00f3n y Activaci\u00f3n","text":"<ul> <li>Las im\u00e1genes se transfieren manualmente mediante scripts basados en <code>rsync</code>.</li> <li>El despliegue implica detener y reiniciar manualmente los servicios.</li> <li>Se asume un downtime aproximado de 10 minutos.</li> <li>Este mecanismo no es deseable para el producto final y ser\u00e1 reemplazado en fases posteriores.</li> </ul>"},{"location":"es/6_production/#4-gestion-de-recursos-y-fallos-conocidos","title":"4. Gesti\u00f3n de Recursos y Fallos Conocidos","text":""},{"location":"es/6_production/#41-saturacion-de-recursos","title":"4.1 Saturaci\u00f3n de Recursos","text":"<ul> <li>La saturaci\u00f3n de CPU, GPU o memoria es un escenario esperado.</li> <li>El sistema no implementa mecanismos autom\u00e1ticos de throttling ni degradaci\u00f3n controlada.</li> </ul>"},{"location":"es/6_production/#42-disponibilidad-de-fuentes","title":"4.2 Disponibilidad de Fuentes","text":"<ul> <li> <p>El sistema puede fallar si:</p> </li> <li> <p>La fuente no est\u00e1 disponible durante la inicializaci\u00f3n.</p> </li> <li>La fuente deja de estar disponible tras haberse conectado al menos una vez.</li> <li>El comportamiento ante estos escenarios no est\u00e1 completamente determinado y forma parte de la deuda t\u00e9cnica del MVP.</li> </ul>"},{"location":"es/6_production/#43-criterios-de-fallo-del-proceso","title":"4.3 Criterios de Fallo del Proceso","text":"<ul> <li>Los criterios formales para terminar el proceso (fatal vs recoverable) no est\u00e1n definidos.</li> <li> <p>Requiere an\u00e1lisis adicional para:</p> </li> <li> <p>OOM en GPU o RAM.</p> </li> <li>Bloqueos del pipeline.</li> <li>Degradaci\u00f3n sostenida (frame drop, latencia).</li> </ul>"},{"location":"es/6_production/#5-pipeline-de-memoria-y-zero-copy","title":"5. Pipeline de Memoria y Zero-Copy","text":"<ul> <li>No se garantiza zero-copy end-to-end.</li> <li> <p>Limitaciones principales:</p> </li> <li> <p>Modificaci\u00f3n de frames en CPU durante la fase de dibujo (OpenCV), especialmente en segmentaci\u00f3n.</p> </li> <li>Modelos que requieren entrada en <code>float32</code>.</li> <li>Copias impl\u00edcitas realizadas por PyTorch durante inferencia.</li> <li>En Jetson, la arquitectura de memoria unificada reduce el costo respecto a x86, pero no elimina las copias.</li> <li>Conclusi\u00f3n: zero-copy solo es posible dentro de los l\u00edmites impuestos por Python + CUDA + PyTorch.</li> </ul>"},{"location":"es/6_production/#6-metricas-y-observabilidad","title":"6. M\u00e9tricas y Observabilidad","text":"<p>Las m\u00e9tricas de producci\u00f3n no est\u00e1n completamente definidas en el MVP.</p> <p>Este documento deja abierto determinar:</p> <ul> <li>Qu\u00e9 m\u00e9tricas ser\u00e1n recolectadas (FPS, frame drop, latencia, uso de recursos).</li> <li>Nivel de granularidad (global vs por stream).</li> <li>Medio de exposici\u00f3n (logs, stdout, endpoint).</li> </ul>"},{"location":"es/6_production/#7-configuracion-y-seguridad","title":"7. Configuraci\u00f3n y Seguridad","text":"<ul> <li>La pol\u00edtica de configuraci\u00f3n (build-time vs runtime) no est\u00e1 definida.</li> <li>Aspectos de seguridad (credenciales, secretos, autenticaci\u00f3n de fuentes) quedan fuera de scope del MVP.</li> </ul>"},{"location":"es/6_production/#8-limitaciones-del-mvp","title":"8. Limitaciones del MVP","text":"<ul> <li>Operaci\u00f3n manual.</li> <li>Downtime aceptado.</li> <li>Sin HA.</li> <li>Sin auto-recovery robusto.</li> <li>Observabilidad limitada.</li> </ul> <p>Estas limitaciones son conocidas y se consideran aceptables \u00fanicamente en el contexto de MVP.</p>"},{"location":"es/7_translation_table/","title":"Tabla de Traducci\u00f3n","text":"<p>Tabla que muestra por termino su significado y equivalente o nombre original en ingl\u00e9s </p> Espa\u00f1ol Ingl\u00e9s definitivo tasa de p\u00e9rdida de frames frame drop rate latencia de procesamiento processing latency ejecuci\u00f3n de run run execution"},{"location":"es/","title":"Motor de Inferencia de streams","text":"<p>Este documento define el alcance funcional y operativo del Stream Inference Engine. Describe el problema que resuelve, las restricciones impuestas por el contexto del MVP, los supuestos t\u00e9cnicos bajo los que opera y los l\u00edmites expl\u00edcitos de responsabilidad del sistema. No detalla implementaci\u00f3n interna ni decisiones de ingenier\u00eda espec\u00edficas, sino el marco contractual y operativo bajo el cual el sistema debe evaluarse.</p>"},{"location":"es/#proposito-del-sistema-problem-space","title":"Prop\u00f3sito del sistema (Problem Space)","text":"<ul> <li>Procesar m\u00faltiples streams de video concurrentes en tiempo real.</li> <li>Ejecutar inferencia sobre streams de video y generar salida visual.</li> <li>Publicar el resultado como stream de video, no como metadata estructurada.</li> <li>Operar en contexto productivo B2B, no como servicio gen\u00e9rico o self-service.</li> <li>Definir \u201ctiempo real\u201d mediante frame-drop rate (&lt; 5% respecto a la entrada).</li> <li>Se considera tiempo real cuando el frame-drop rate &lt; 5% respecto al framerate de entrada.</li> </ul>"},{"location":"es/#supuestos-del-contexto-assumptions","title":"Supuestos del contexto (Assumptions)","text":"<ul> <li>Las fuentes de video son c\u00e1maras remotas externas.</li> <li>Las fuentes exponen streams v\u00eda RTSP.</li> <li>El codec de entrada es H.264 (impuesto por el contexto del MVP).</li> <li>El protocolo de salida es RTSP.</li> <li>La inferencia se ejecuta prioritariamente sobre GPU.</li> <li>El sistema se despliega sobre plataformas x86_64 y arm64 (Jetson).</li> <li>Varias decisiones t\u00e9cnicas fueron impuestas por el contexto organizacional del MVP.</li> </ul>"},{"location":"es/#restricciones-no-negociables-constraints","title":"Restricciones no negociables (Constraints)","text":"<ul> <li>El sistema no controla el protocolo ni el codec de las fuentes.</li> <li>El sistema no controla la disponibilidad ni estabilidad de la red.</li> <li>La salida debe mantenerse en tiempo real bajo la m\u00e9trica definida.</li> <li>La salida es exclusivamente video con overlays; no se emite metadata adicional.</li> <li>Se priorizan continuidad, estabilidad y latencia por sobre optimizaci\u00f3n extrema.</li> <li>El servicio se entrega directamente en un contexto B2B (sin elasticidad autom\u00e1tica).</li> </ul>"},{"location":"es/#postura-operativa-del-sistema","title":"Postura operativa del sistema","text":"<ul> <li>El sistema valida la alcanzabilidad de la fuente antes de iniciar el procesamiento.</li> <li>Ante p\u00e9rdida de conexi\u00f3n, el sistema reintenta seg\u00fan configuraci\u00f3n.</li> <li>Si se excede el n\u00famero de reintentos, el stream se considera fallido.</li> <li>En el estado actual del MVP no existen mecanismos autom\u00e1ticos de limitaci\u00f3n de carga.</li> <li>Solo se ofrecen configuraciones consideradas aceptables para no degradar performance.</li> </ul>"},{"location":"es/#limites-explicitos-de-responsabilidad-responsibility-boundaries","title":"L\u00edmites expl\u00edcitos de responsabilidad (Responsibility Boundaries)","text":"<ul> <li>El sistema no garantiza la calidad ni correcci\u00f3n de la inferencia.</li> <li>La accuracy es responsabilidad del modelo cargado.</li> <li>El sistema no garantiza accesibilidad de las fuentes.</li> <li>Fallas de red est\u00e1n fuera de scope.</li> <li>Fallas del emisor del stream est\u00e1n fuera de scope.</li> <li>Fallas de sistemas consumidores de la salida est\u00e1n fuera de scope.</li> <li>El sistema no garantiza optimizaci\u00f3n \u00f3ptima de recursos en el estado actual del MVP.</li> </ul>"},{"location":"es/#estado-actual-del-sistema","title":"Estado actual del sistema","text":"<ul> <li>El sistema se encuentra en estado MVP.</li> <li>Demuestra viabilidad funcional end-to-end:</li> <li>inicio de streams</li> <li>ejecuci\u00f3n de inferencia</li> <li>visualizaci\u00f3n del resultado en un frontend</li> <li>Existen limitaciones conocidas de consumo de recursos (RAM, uso parcial de NVENC/NVDEC).</li> <li>Estas limitaciones se reconocen como deuda t\u00e9cnica y no como fallas funcionales.</li> </ul>"}]}