{"config":{"lang":["en","es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"1_overview/","title":"Overview","text":"<p>This section describes the nature of the Stream Inference Engine, its position within the overall architecture, and the responsibilities it assumes in stream processing. It defines the functional boundaries of the system, its integration model with other services, configuration and state management, and behavior under failure. Finally, it contextualizes the current state as an MVP and the planned evolution paths oriented toward scalability and observability improvements.</p>"},{"location":"1_overview/#system-nature","title":"System Nature","text":"<ul> <li>The system operates as an inference engine that runs video processing runtimes.</li> <li>It executes exclusively business-defined rules.</li> <li>It does not make autonomous decisions or apply its own business logic.</li> <li>It provides flexibility for per-client custom inference.</li> </ul>"},{"location":"1_overview/#role-within-the-architecture","title":"Role Within the Architecture","text":"<ul> <li>It is part of a broader microservice architecture.</li> <li>It implements three main logical modules:</li> <li>Inference module</li> <li>Configuration and model management module</li> <li>Drawing module</li> <li>These modules are integrated into an orchestrator called <code>inference_engine</code>.</li> <li>The engine retrieves jobs to execute through database polling.</li> <li>The API acts as an intermediary between other services and the engine.</li> </ul>"},{"location":"1_overview/#core-responsibilities","title":"Core Responsibilities","text":"<ul> <li>Run inference on RTSP video streams.</li> <li>Apply previously defined rules over inference results.</li> <li>Draw results directly onto frames.</li> <li>Publish the resulting stream via RTSP.</li> <li>Maintain stream execution regardless of whether consumers are connected.</li> </ul>"},{"location":"1_overview/#configuration-and-model-management","title":"Configuration and Model Management","text":"<ul> <li>Configurations and model weights are managed by the API.</li> <li>The engine consumes only previously loaded configurations and resources.</li> <li>Enables custom inference based on per-client specific configurations.</li> </ul>"},{"location":"1_overview/#state-management-and-persistence","title":"State Management and Persistence","text":"<ul> <li>Persists short-term state required to execute rules across frames.</li> <li>State is used to detect history-dependent events.</li> <li>Events may result in visual drawing or future notification.</li> <li>Long-term inference history is not persisted.</li> </ul>"},{"location":"1_overview/#exposure-and-integration","title":"Exposure and Integration","text":"<ul> <li>Authentication and authorization are out of scope.</li> <li>Access control is delegated to other architecture layers.</li> <li>The system exposes RTSP streams without assuming the existence of active consumers.</li> <li>Other services may consume streams or events independently.</li> </ul>"},{"location":"1_overview/#error-handling-and-run-execution","title":"Error Handling and Run Execution","text":"<ul> <li>If a source is not reachable at startup, the run is marked as failed.</li> <li>Upon connection loss, configurable retries are applied.</li> <li>If retries are exhausted, the run is marked as failed.</li> <li>Implementation errors are considered non-recoverable.</li> <li>Run state is persisted in the database.</li> </ul>"},{"location":"1_overview/#current-system-state","title":"Current System State","text":"<ul> <li>The system is in MVP state.</li> <li>The primary focus was validating technical and business feasibility.</li> <li>Stress tests were conducted to identify performance limits.</li> <li>The system has room for improvement with additional development.</li> </ul>"},{"location":"1_overview/#planned-evolution","title":"Planned Evolution","text":"<ul> <li>Evolution prioritizes system scalability.</li> <li>Scalability will impact throughput and latency.</li> <li>Precise metric measurement is currently limited.</li> <li>The two-pipeline architecture affects frame traceability.</li> </ul>"},{"location":"2_architecture/","title":"Architecture","text":"<p>This section describes the structural decomposition of the system into conceptual modules, the data and control flow between them, and the communication mechanisms used. It defines critical external dependencies (hardware, drivers, and frameworks), the current scalability limits under the target hardware, and the metrics considered for evaluating behavior. It also delimits the extensibility of components and the ownership model within the MVP scope.</p>"},{"location":"2_architecture/#conceptual-modules","title":"Conceptual Modules","text":"<p>The system is organized into two planes: control and data.</p> <p>Control Plane - API: Interface to other services. Manages configurations, model weights, and database persistence. - Engine: Central orchestrator. Manages the run lifecycle through database polling and coordinates Workers and Stream Manager via IPC. - Database: State mediator between API and Engine.</p> <p>Data Plane - Workers (0..N): Independent processes, one per active stream. Each Worker runs an Inference Pipeline that ingests the RTSP source, applies inference and drawing over frames, encodes them as H264, and writes to shared memory. - Shared Memory: Frame transfer mechanism between Workers and Stream Manager. - Stream Manager: Central process of the data plane. Contains the Factory Pipeline, which reads frames from shared memory and publishes them through the RTSP server.</p> <pre><code>flowchart TB\n    subgraph CP[\"Control Plane\"]\n        direction TB\n        API[\"REST API\"] &lt;--&gt; DB[(SQLite)] &lt;--&gt; ENG[\"Engine &lt;br/&gt; (Lifecycle \u00b7 Polling \u00b7 IPC)\"]\n    end\n\n    subgraph DP[\"Data Plane\"]\n        direction LR\n        SRC[\"RTSP Sources &lt;br/&gt; (0..N)\"]\n        IP[\"Workers (0..N) &lt;br/&gt; Inference Pipeline &lt;br/&gt; (GStreamer + DeepStream) &lt;br/&gt; Ingestion \u00b7 Inference &lt;br/&gt; Drawing \u00b7 Encode\"]\n        SHM[/\"Shared Memory\"/]\n        FP[\"Stream Manager &lt;br/&gt; Factory Pipeline &lt;br/&gt; RTSP Server\"]\n        OUT[\"RTSP Consumers &lt;br/&gt; (0..N)\"]\n        SRC --&gt; IP\n        IP --&gt;|\"H264 frames\"| SHM --&gt; FP --&gt; OUT\n    end\n\n    ENG -.-&gt; DP\n\n    classDef control fill:#dbeafe,stroke:#3b82f6,color:#1e3a5f\n    classDef data fill:#dcfce7,stroke:#22c55e,color:#14532d\n    classDef external fill:#fef9c3,stroke:#eab308,color:#422006\n    classDef memory fill:#fce7f3,stroke:#ec4899,color:#500724\n\n    class API,DB,ENG control\n    class IP,FP data\n    class SRC,OUT external\n    class SHM memory</code></pre>"},{"location":"2_architecture/#control-flow-and-communication","title":"Control Flow and Communication","text":"<ul> <li>API and Engine communicate exclusively through the database.</li> <li>Engine coordinates Workers and Stream Manager via IPC (stdin/stdout) with JSON messages.</li> <li>Processed frames are transferred between Workers and Stream Manager through shared memory.</li> <li>All components must be operational to execute a run; if any fails, the run does not complete.</li> </ul>"},{"location":"2_architecture/#external-dependencies-and-hardware","title":"External Dependencies and Hardware","text":"<ul> <li>Strong dependency on GPU architecture: Tesla V100, NVIDIA T4, Jetson.</li> <li>Required libraries and frameworks: CUDA 12.6, cuDNN, DeepStream 7.1, pyds, GStreamer plugins, specific NVIDIA drivers, YOLO, PyTorch, TensorRT.</li> <li>API and Engine depend on the database (currently local).</li> <li>Modules depend on available hardware to meet expected performance.</li> </ul>"},{"location":"2_architecture/#scalability-and-conceptual-limits","title":"Scalability and Conceptual Limits","text":"<ul> <li>Current limit: 6 1080p streams with full-frame inference using a YOLO nano model.</li> <li>Estimated potential: ~10 streams with resource optimization and without scaling hardware.</li> <li>Current consumption for 6 streams: 16 GB RAM, &lt;12 GB VRAM, Tesla V100 GPU, 6 CPU cores.</li> <li>Scaling via additional hardware is faster but costly; code optimization can improve efficiency without increasing resources.</li> </ul>"},{"location":"2_architecture/#conceptual-metrics","title":"Conceptual Metrics","text":"<ul> <li>Frame drop rate, end-to-end latency, and processing latency are measured.</li> <li>The Factory Pipeline measures only end-to-end latency; frame drop is not representative given that it re-consumes the same frames from shared memory.</li> <li>Real measurement is limited due to loss of frame identity between pipelines.</li> <li>Future improvements: add per-frame metadata for traceability between pipelines and precise throughput and latency measurement.</li> </ul>"},{"location":"2_architecture/#extensibility-and-ownership","title":"Extensibility and Ownership","text":"<ul> <li>Modules such as rule validation, rule evaluation, model loading, and drawing are extensible, though they require developer intervention.</li> <li>Engine orchestrates all modules; API almost fully decouples interaction through the database.</li> <li>The conceptual design allows scaling and modifying functionality without affecting other modules, within MVP boundaries.</li> </ul>"},{"location":"3_performance/","title":"Performance","text":"<p>This section describes the performance evaluation model of the system in the MVP context. It defines the validity scope, the formal criteria for \"real time\", the metrics used and their limitations. It does not constitute a comparative benchmark nor does it expose maximum system capacity.</p>"},{"location":"3_performance/#scope-and-validity-context","title":"Scope and Validity Context","text":"<p>The considerations in this document are valid only under a controlled environment equivalent to the one used during MVP development:</p> <ul> <li>x86_64 architecture</li> <li>NVIDIA datacenter-class GPU</li> <li>Multi-core CPU</li> <li>Compatible CUDA + TensorRT + DeepStream</li> <li>Homogeneous streams</li> <li>Resolutions between 480p and 1080p</li> <li>Input FPS in the 25\u201330 range</li> </ul> <p>Any variation in hardware, drivers, models, or configuration invalidates direct extrapolation of the described behavior.</p>"},{"location":"3_performance/#load-definition","title":"Load Definition","text":"<p>Load is understood as the number of streams processed in parallel, each associated with one or more inference models.</p> <p>Increasing load impacts:</p> <ul> <li>Effective FPS per stream</li> <li>Aggregate frame drop rate</li> <li>Average latencies</li> <li>GPU and memory utilization</li> </ul> <p>The system exhibits progressive degradation when resource contention prevents sustaining real-time processing for all active streams.</p>"},{"location":"3_performance/#acceptance-criteria-real-time","title":"Acceptance Criteria (Real-Time)","text":"<p>System state is classified according to the aggregate frame drop rate:</p> <ul> <li>0% \u2013 1%   Optimal real time within MVP scope.</li> <li>1% \u2013 5%   Acceptable performance for functional validation.</li> <li>&gt; 5%   The service is no longer considered real time in the MVP context.</li> </ul> <p>These thresholds are defined for internal technical validation purposes and do not represent production SLAs.</p> <p>The real-time state of the system is determined using the aggregate value corresponding to <code>workers_frame_drop_rate_avg</code>.</p>"},{"location":"3_performance/#measurement-methodology-high-level","title":"Measurement Methodology (High Level)","text":"<ul> <li>Metrics exclude warm-up and pipeline creation.</li> <li>Captured only under stable steady state.</li> <li>Stream homogeneity is assumed.</li> <li>Per-stream metrics are averaged over time.</li> <li>System metrics are periodic aggregations of all per-stream metrics.</li> <li>No outlier detection or removal is performed.</li> </ul> <p>The goal is to characterize general behavior under load, not to perform exhaustive statistical analysis.</p>"},{"location":"3_performance/#metrics-used","title":"Metrics Used","text":"<p>The metrics used are aggregates persisted by the Engine and derived from the <code>system_metrics</code> model.</p> <p>The following are considered:</p>"},{"location":"3_performance/#worker-pipelines","title":"Worker pipelines","text":"<ul> <li><code>workers_fps_avg</code></li> <li><code>workers_frame_drop_rate_avg</code></li> <li><code>workers_processing_latency_avg_ms</code></li> <li><code>workers_e2e_latency_avg_ms</code></li> </ul>"},{"location":"3_performance/#factory-pipelines","title":"Factory pipelines","text":"<ul> <li><code>factories_fps_avg</code></li> <li><code>factories_e2e_latency_avg_ms</code></li> </ul>"},{"location":"3_performance/#system-resources","title":"System resources","text":"<ul> <li><code>gpu_compute_percent</code></li> <li><code>gpu_memory_percent</code></li> <li><code>ram_percent</code></li> <li><code>cpu_percent</code></li> </ul> <p>Frame drop rate is the primary evaluation metric, as it directly represents the system's inability to sustain the frame flow in real time.</p>"},{"location":"3_performance/#behavior-under-increasing-load","title":"Behavior Under Increasing Load","text":"<p>As the number of parallel streams increases:</p> <ul> <li>GPU usage tends to grow until approaching the saturation point.</li> <li>Frame drop rate increases non-linearly.</li> <li>Average end-to-end latency increases due to resource contention.</li> <li>Effective FPS per stream may degrade progressively.</li> </ul> <p>The degradation point is not a theoretical property of the hardware, but a consequence of the current implementation state.</p> <p>The quantitative limit of sustainable streams is not published in this document.</p>"},{"location":"3_performance/#measurement-limitations","title":"Measurement Limitations","text":"<ul> <li>Jitter is not measured.</li> <li>Fairness between streams is not measured.</li> <li>Real end-to-end latency with full frame traceability is not calculated.</li> <li>Inference accuracy or quality is not evaluated.</li> <li>Deep statistical analysis is not performed.</li> </ul> <p>Loss of frame identity between pipelines prevents exact calculation of real end-to-end latency.</p>"},{"location":"3_performance/#implications-and-expected-evolution","title":"Implications and Expected Evolution","text":"<p>The current operational limit of the MVP is determined primarily by implementation efficiency and resource usage, not by structural architectural constraints.</p> <p>Improvements in:</p> <ul> <li>Memory management</li> <li>Reduction of unnecessary copies</li> <li>Dynamic load control</li> <li>More precise metric instrumentation</li> </ul> <p>should allow increasing effective capacity while maintaining the defined real-time thresholds.</p> <p>Future evolution prioritizes per-stream horizontal scalability and explicit degradation control.</p>"},{"location":"3_performance/#out-of-scope","title":"Out of Scope","text":"<p>This chapter does not cover:</p> <ul> <li>Exact maximum system capacity</li> <li>Detailed benchmarking procedures</li> <li>Comparisons with other implementations</li> <li>Production SLAs</li> <li>Multi-node scalability</li> </ul>"},{"location":"4_engineering-decisions/","title":"Engineering Decisions","text":"<p>This chapter documents the structural technical decisions that defined the current system architecture, including motivations, environment constraints, discarded alternatives, and accepted costs. It exposes how the MVP priorities\u2014stability, functional viability, and extensibility\u2014conditioned choices in process separation, GPU usage, memory management, metrics, and decoupling between components, as well as the technical implications these decisions generate for the future evolution of the engine.</p>"},{"location":"4_engineering-decisions/#1-technical-goals-and-priorities","title":"1. Technical Goals and Priorities","text":"<ul> <li>Primary objectives</li> <li>Best-effort zero-copy throughout the pipeline.</li> <li>High extensibility in the number of models, weights, and inference tasks.</li> <li>Always maintain a real-time output.</li> <li>Prioritization</li> <li>Functional viability was prioritized over absolute performance.</li> <li>The system was conceived as an MVP aimed at validating technical and business feasibility.</li> <li>Discarded or deferred objectives</li> <li>Full end-to-end zero-copy.</li> <li>Completely accurate performance metrics.</li> <li>Broad inference framework support from the start.</li> <li>Initial framework scope</li> <li>Primary focus on YOLO.</li> <li>Specific models such as Anomalib were explicitly deferred.</li> </ul>"},{"location":"4_engineering-decisions/#2-process-and-pipeline-separation","title":"2. Process and Pipeline Separation","text":"<ul> <li>Primary motivation</li> <li>Critical incompatibilities between CUDA, Python processing libraries, GStreamer, PyGObject, and DeepStream within the same runtime.</li> <li>Observed problems</li> <li>Python process crashes without exception capture.</li> <li>Failures when:<ul> <li>Initializing GStreamer pipelines.</li> <li>Accessing GstBuffer from transforms.</li> <li>Starting the Python debugger with GStreamer active.</li> <li>Creating multiple pipelines in the same runtime.</li> <li>Using hardware encoder and decoder simultaneously.</li> </ul> </li> <li>Technical hypothesis</li> <li>Non-reentrant CUDA libraries.</li> <li>Internal memory state corruption when loaded multiple times in the same runtime.</li> <li>Decision</li> <li>Strict separation into independent processes using <code>subprocess</code>.</li> <li>Resulting topology</li> </ul> <pre><code>Inference Engine\n\u251c\u2500 Stream Manager Bridge (IPC)\n\u251c\u2500 Stream Manager\n\u2502  \u251c\u2500 RTSP Server\n\u2502  \u251c\u2500 Factory Pipeline\n\u2502  \u2514\u2500 Workers (0..N)\n\u2514\u2500 Stream Workers\n   \u2514\u2500 Inference Pipeline (0..N)\n</code></pre> <ul> <li>Communication based on IPC (stdin/stdout) and shared memory.</li> <li>Factory Pipeline / Inference Pipeline separation</li> <li>Factory Pipeline (Stream Manager): reads from shared memory (shmsrc) and feeds the RTSP server.</li> <li>Inference Pipeline (Workers): RTSP ingestion \u2192 inference \u2192 drawing \u2192 H264 encode \u2192 shared memory (shmsink).</li> <li>Additional motivation</li> <li>Incompatibility between RTSP server runtime and DeepStream libraries.</li> <li>Runtime isolation avoids dependency collisions (including previous versions such as Savant).</li> <li>Accepted costs</li> <li>Higher latency.</li> <li>Longer initialization time.</li> <li>Greater operational complexity.</li> <li>Benefit</li> <li>System stability and consistent execution.</li> </ul>"},{"location":"4_engineering-decisions/#3-choice-of-gstreamer-deepstream-and-frameworks","title":"3. Choice of GStreamer, DeepStream, and Frameworks","text":"<ul> <li>Origin of the decision</li> <li>GStreamer was an externally imposed requirement.</li> <li>Initial implementation discarded</li> <li>OpenCV + NumPy on CPU: functionally viable, not viable for GPU zero-copy.</li> <li>Motivation for the change</li> <li>Need to consume RTSP and generate frames directly on GPU.</li> <li>Selected stack</li> <li>GStreamer + DeepStream for GPU-native ingestion and processing.</li> <li>TensorRT as part of the NVIDIA runtime (available through the DeepStream SDK and CUDA Toolkit, not as a declared project dependency).</li> <li>Inference frameworks</li> <li>YOLO as the primary framework due to:</li> <li>Abundance of models.</li> <li>Python-first approach.</li> <li>High extensibility without modifying the system core.</li> <li>Protocol</li> <li>RTSP imposed by external sources (domain problem constraint).</li> </ul>"},{"location":"4_engineering-decisions/#4-memory-and-frame-management","title":"4. Memory and Frame Management","text":"<ul> <li>Guiding principle</li> <li>Minimize frame copies (CPU\u2194GPU and GPU\u2194GPU).</li> <li>Benefits</li> <li>Lower RAM and VRAM consumption.</li> <li>Better throughput.</li> <li>Lower frame drop rate.</li> <li>Critical decision</li> <li>Accept loss of frame identity between pipelines.</li> <li>Justification</li> <li>Preserving identity requires:</li> <li>Additional shared memory allocation.</li> <li>Persistent metadata header per frame.</li> <li>High development effort incompatible with MVP timelines.</li> <li>Current consequence</li> <li>The Inference Pipeline generates processed frames at ~25 FPS in shared memory.</li> <li>The Factory Pipeline consumes shared memory at ~45 FPS, retransmitting the same frame multiple times to the RTSP server.</li> <li>Specific technical decisions</li> <li>Use of DeepStream to generate frames directly on GPU.</li> <li>Use of CuPy to capture GPU references in Python.</li> <li>Use of PyTorch for its device-decoupled tensor abstraction.</li> <li>Direct drawing on frame whenever possible.</li> <li>Use of metadata + <code>nvdsosd</code> for text and overlays on GPU.</li> </ul>"},{"location":"4_engineering-decisions/#5-concurrency-load-and-frame-drop","title":"5. Concurrency, Load, and Frame Drop","text":"<ul> <li>Stream limit</li> <li>Not explicitly imposed by code.</li> <li>Determined empirically by resource consumption.</li> <li>Frame drop</li> <li>Accepted as the primary metric.</li> <li>It is the most faithful metric given current limitations.</li> <li>Backpressure</li> <li>Implemented through queues that discard frames before processing.</li> <li>Metrics</li> <li>Output FPS sufficient to infer frame drop rate.</li> <li>Load control</li> <li>Not implemented.</li> <li>If implemented, it would be based on resource consumption, not performance metrics.</li> </ul>"},{"location":"4_engineering-decisions/#6-api-database-and-decoupling","title":"6. API, Database, and Decoupling","text":"<ul> <li>Decision</li> <li>Decouple API and Inference Engine through the database.</li> <li>Motivation</li> <li>Avoid state synchronization between heterogeneous runtimes.</li> <li>Allow multiple inference engine instances in the future.</li> <li>Current state</li> <li>Local database.</li> <li>Future support for horizontal scaling not implemented.</li> </ul>"},{"location":"4_engineering-decisions/#7-observability-deferred","title":"7. Observability (Deferred)","text":"<ul> <li>State</li> <li>Partially addressed.</li> <li>Limited by loss of frame identity.</li> <li>Technical debt</li> <li>Precise per-frame latency metrics.</li> <li>Full pipeline traceability.</li> </ul>"},{"location":"4_engineering-decisions/#8-scope-decisions","title":"8. Scope Decisions","text":"<ul> <li>Excluded from MVP</li> <li>Advanced load control.</li> <li>Exhaustive metrics.</li> <li>Full lifecycle management.</li> <li>Reasoning</li> <li>Not critical for validating business feasibility.</li> </ul>"},{"location":"4_engineering-decisions/#9-portability-and-hardware","title":"9. Portability and Hardware","text":"<ul> <li>Supported platforms</li> <li>x86: Tesla V100, T4.</li> <li>Jetson: JetPack 6.1.</li> <li>Objective</li> <li>Viability in both edge and datacenter environments.</li> <li>Result</li> <li>Requirement met, though with pending technical debt.</li> </ul>"},{"location":"4_engineering-decisions/#10-future-evolution","title":"10. Future Evolution","text":"<ul> <li>Conditioning decisions</li> <li>Decoupled pipeline architecture with shared memory.</li> <li>Replaceable components</li> <li>Inference frameworks.</li> <li>Models and weights.</li> <li>Pending review</li> <li>Performance optimization.</li> <li>Library compatibility.</li> <li>Observability and metrics.</li> </ul>"},{"location":"5_limitations/","title":"Limitations","text":"<p>This section explicitly states the technical constraints observed in the current MVP state, differentiating limits derived from implementation, environment, and scope priorities from structural design limitations. It defines the current operational boundaries of the system in terms of concurrency, zero-copy, metrics, scalability, and observability, establishing which behaviors are inherent to the present state and which constitute planned technical debt for future evolution.</p>"},{"location":"5_limitations/#stream-concurrency-limit-mvp","title":"Stream Concurrency Limit (MVP)","text":"<ul> <li>The MVP currently supports up to 6 concurrent streams with acceptable performance.</li> <li>This limit arises from aggregate resource consumption (RAM, VRAM, hardware encoder/decoder, and GPU compute), not from a logical system constraint.</li> <li>The limit is not imposed by configuration nor hardcoded; it is the point at which the frame drop rate exceeds acceptable real-time thresholds.</li> <li>With additional technical optimization, the system is estimated to scale to ~10 streams on the same hardware, depending on:</li> <li>resolution,</li> <li>input FPS,</li> <li>model complexity,</li> <li>type of inference task.</li> <li>Even with optimization, the hardware imposes a practical ceiling that limits scalability without increasing resources.</li> </ul>"},{"location":"5_limitations/#zero-copy-limitations","title":"Zero-Copy Limitations","text":"<ul> <li>A complete zero-copy scheme throughout the entire pipeline has not been achieved.</li> <li>On Jetson platforms, although RAM and VRAM share the same chip, full zero-copy is not viable within the current limits of:</li> <li>Python,</li> <li>CUDA,</li> <li>PyTorch,</li> <li>DeepStream,</li> <li>GStreamer.</li> <li>During the drawing phase:</li> <li>certain operations (e.g., segmentation) are simpler to implement on CPU using OpenCV,</li> <li>which introduces explicit or implicit memory copies.</li> <li>Some models require <code>float32</code> input, implying additional conversions and copies.</li> <li>In practice, inference frameworks perform internal copies, so this behavior is expected.</li> <li>Conclusion: a best-effort zero-copy approach is applied, keeping frames on GPU as long as possible, but accepting copies when necessary.</li> </ul>"},{"location":"5_limitations/#metrics-and-observability-limitations","title":"Metrics and Observability Limitations","text":"<ul> <li>The architecture based on two pipelines decoupled via shared memory causes loss of frame identity between pipelines.</li> <li>This prevents precise measurement of:</li> <li>real end-to-end latency,</li> <li>effective throughput,</li> <li>frame-to-frame correlation between input and output.</li> <li>Currently:</li> <li>frame drop rate is used as the primary metric to evaluate real-time behavior,</li> <li>other metrics are approximated.</li> <li>Implementing full traceability would require:</li> <li>extending the shared memory schema with metadata headers,</li> <li>or introducing explicit inter-process synchronization mechanisms.</li> <li>Given the MVP scope, this improvement is considered technical debt.</li> </ul>"},{"location":"5_limitations/#mvp-scope-limitations","title":"MVP Scope Limitations","text":"<ul> <li>The system was developed as an MVP oriented toward technical and business validation, not as a final product.</li> <li>Not implemented:</li> <li>resource-based load control mechanisms,</li> <li>dynamic run execution limits,</li> <li>automatic scaling.</li> <li>The number of supported inference frameworks is limited:</li> <li>YOLO as the primary framework,</li> <li>limited support for Anomalib.</li> <li>Inference quality depends exclusively on the loaded model; the system does not apply semantic validation of results.</li> <li>The system does not guarantee:</li> <li>source accessibility,</li> <li>network stability,</li> <li>input image quality,</li> <li>behavior of stream consumers.</li> </ul>"},{"location":"5_limitations/#resolved-technical-debt","title":"Resolved Technical Debt","text":"<ul> <li>The full frame cache in CPU within the drawing module was eliminated, reducing the excessive RAM consumption observed in previous versions. The module retains cache of inference results and drawing metadata between frames, but no longer stores frame copies in CPU memory.</li> </ul>"},{"location":"5_limitations/#deferred-improvements","title":"Deferred Improvements","text":"<p>The following limitations are considered addressable in future development phases:</p> <ul> <li>Additional optimization of RAM and VRAM memory consumption.</li> <li>Improved run lifecycle and greater resilience to failures.</li> <li>Implementation of resource metrics-based load control.</li> <li>Improved metrics and frame traceability.</li> <li>Expanded framework and model type support.</li> </ul>"},{"location":"6_production/","title":"Production (MVP)","text":"<p>This section delimits the operational behavior of the system in its current state under a controlled single-node production environment. It defines execution assumptions, deployment constraints, resilience limits, and the absence of advanced automation, making explicit what minimum guarantees exist and what aspects remain as technical debt. It does not constitute a definitive production design nor establish formal availability commitments or SLAs.</p>"},{"location":"6_production/#1-production-scope","title":"1. Production Scope","text":"<ul> <li>Single-node architecture.</li> <li>1 GPU per host is assumed.</li> <li>High availability (HA) and multi-node deployment are not contemplated.</li> <li>Horizontal scalability is out of MVP scope.</li> <li>The practical stream limit is conditioned by both resource consumption and architectural pipeline constraints.</li> </ul>"},{"location":"6_production/#2-execution-architecture","title":"2. Execution Architecture","text":""},{"location":"6_production/#21-engine-lifecycle","title":"2.1 Engine Lifecycle","text":"<ul> <li>The engine runs persistently and does not terminate due to inactivity.</li> <li>It waits indefinitely for <code>run</code> requests.</li> <li>If no source connects during initialization, the system is not considered functionally deployed and remains in a waiting state.</li> <li>In that scenario, it is assumed that <code>runs</code> will be requested again externally.</li> <li>This behavior is subject to change in future iterations.</li> </ul>"},{"location":"6_production/#22-retry-strategy","title":"2.2 Retry Strategy","text":"<ul> <li>A fixed retry policy is used.</li> <li>No exponential backoff exists.</li> <li>The system is considered UP even if only one source has connected successfully.</li> <li>This logic is one of the weakest points of the MVP and requires additional engineering for real production.</li> </ul>"},{"location":"6_production/#3-deployment","title":"3. Deployment","text":""},{"location":"6_production/#31-supported-targets","title":"3.1 Supported Targets","text":"<ul> <li><code>x86_64</code> with GPU.</li> <li><code>arm64</code> (Jetson).</li> </ul>"},{"location":"6_production/#32-build-process","title":"3.2 Build Process","text":"<ul> <li> <p>Image builds are performed on development hosts:</p> </li> <li> <p>x86 host for <code>x86_64</code> images.</p> </li> <li>Development Jetson for <code>arm64</code> images.</li> <li>No automated CI/CD pipeline exists.</li> </ul>"},{"location":"6_production/#33-distribution-and-activation","title":"3.3 Distribution and Activation","text":"<ul> <li>Images are transferred manually via <code>rsync</code>-based scripts.</li> <li>Deployment involves manually stopping and restarting services.</li> <li>An approximate 10-minute downtime is assumed.</li> <li>This mechanism is not desirable for the final product and will be replaced in later phases.</li> </ul>"},{"location":"6_production/#4-resource-management-and-known-failures","title":"4. Resource Management and Known Failures","text":""},{"location":"6_production/#41-resource-saturation","title":"4.1 Resource Saturation","text":"<ul> <li>CPU, GPU, or memory saturation is an expected scenario.</li> <li>The system does not implement automatic throttling or controlled degradation mechanisms.</li> </ul>"},{"location":"6_production/#42-source-availability","title":"4.2 Source Availability","text":"<ul> <li> <p>The system may fail if:</p> </li> <li> <p>The source is unavailable during initialization.</p> </li> <li>The source becomes unavailable after having connected at least once.</li> <li>Behavior in these scenarios is not fully determined and forms part of the MVP technical debt.</li> </ul>"},{"location":"6_production/#43-process-failure-criteria","title":"4.3 Process Failure Criteria","text":"<ul> <li>Formal criteria for terminating the process (fatal vs recoverable) are not defined.</li> <li> <p>Additional analysis is required for:</p> </li> <li> <p>GPU or RAM OOM.</p> </li> <li>Pipeline deadlocks.</li> <li>Sustained degradation (frame drop, latency).</li> </ul>"},{"location":"6_production/#5-memory-pipeline-and-zero-copy","title":"5. Memory Pipeline and Zero-Copy","text":"<ul> <li>End-to-end zero-copy is not guaranteed.</li> <li> <p>Main limitations:</p> </li> <li> <p>Frame modification on CPU during the drawing phase (OpenCV), especially in segmentation.</p> </li> <li>Models requiring <code>float32</code> input.</li> <li>Implicit copies performed by PyTorch during inference.</li> <li>On Jetson, the unified memory architecture reduces cost compared to x86, but does not eliminate copies.</li> <li>Conclusion: zero-copy is only possible within the limits imposed by Python + CUDA + PyTorch.</li> </ul>"},{"location":"6_production/#6-metrics-and-observability","title":"6. Metrics and Observability","text":"<p>Production metrics are not fully defined in the MVP.</p> <p>This document leaves open the determination of:</p> <ul> <li>Which metrics will be collected (FPS, frame drop, latency, resource usage).</li> <li>Level of granularity (global vs per stream).</li> <li>Exposure mechanism (logs, stdout, endpoint).</li> </ul>"},{"location":"6_production/#7-configuration-and-security","title":"7. Configuration and Security","text":"<ul> <li>Configuration policy (build-time vs runtime) is not defined.</li> <li>Security aspects (credentials, secrets, source authentication) are out of MVP scope.</li> </ul>"},{"location":"6_production/#8-mvp-limitations","title":"8. MVP Limitations","text":"<ul> <li>Manual operation.</li> <li>Accepted downtime.</li> <li>No HA.</li> <li>No robust auto-recovery.</li> <li>Limited observability.</li> </ul> <p>These limitations are known and considered acceptable only in the MVP context.</p>"},{"location":"","title":"Stream Inference Engine","text":"<p>This document defines the functional and operational scope of the Stream Inference Engine. It describes the problem it solves, the constraints imposed by the MVP context, the technical assumptions under which it operates, and the explicit limits of system responsibility. It does not detail internal implementation or specific engineering decisions, but rather the contractual and operational framework under which the system should be evaluated.</p>"},{"location":"#system-purpose-problem-space","title":"System Purpose (Problem Space)","text":"<ul> <li>Process multiple concurrent video streams in real time.</li> <li>Run inference on video streams and generate visual output.</li> <li>Publish the result as a video stream, not as structured metadata.</li> <li>Operate in a B2B production context, not as a generic or self-service offering.</li> <li>Define \"real time\" through frame drop rate (&lt; 5% relative to input framerate).</li> </ul>"},{"location":"#context-assumptions","title":"Context Assumptions","text":"<ul> <li>Video sources are external remote cameras.</li> <li>Sources expose streams via RTSP.</li> <li>Input codec is H.264 (imposed by the MVP context).</li> <li>Output protocol is RTSP.</li> <li>Inference runs primarily on GPU.</li> <li>The system is deployed on x86_64 and arm64 (Jetson) platforms.</li> <li>Several technical decisions were imposed by the organizational context of the MVP.</li> </ul>"},{"location":"#non-negotiable-constraints","title":"Non-Negotiable Constraints","text":"<ul> <li>The system does not control the protocol or codec of the sources.</li> <li>The system does not control network availability or stability.</li> <li>Output must remain real time under the defined metric.</li> <li>Output is exclusively video with overlays; no additional metadata is emitted.</li> <li>Continuity, stability, and latency are prioritized over extreme optimization.</li> <li>The service is delivered directly in a B2B context (without automatic elasticity).</li> </ul>"},{"location":"#system-operational-posture","title":"System Operational Posture","text":"<ul> <li>The system validates source reachability before starting processing.</li> <li>Upon connection loss, the system retries according to configuration.</li> <li>If the retry limit is exceeded, the stream is considered failed.</li> <li>In the current MVP state, there are no automatic load-limiting mechanisms.</li> <li>Only configurations considered acceptable to avoid performance degradation are offered.</li> </ul>"},{"location":"#explicit-responsibility-boundaries","title":"Explicit Responsibility Boundaries","text":"<ul> <li>The system does not guarantee inference quality or correctness.</li> <li>Accuracy is the responsibility of the loaded model.</li> <li>The system does not guarantee source accessibility.</li> <li>Network failures are out of scope.</li> <li>Failures of the stream emitter are out of scope.</li> <li>Failures of output stream consumers are out of scope.</li> <li>The system does not guarantee optimal resource utilization in the current MVP state.</li> </ul>"},{"location":"#current-system-state","title":"Current System State","text":"<ul> <li>The system is in MVP state.</li> <li>It demonstrates end-to-end functional viability:</li> <li>stream startup</li> <li>inference execution</li> <li>result visualization in a frontend</li> <li>Known resource consumption limitations exist (RAM, partial NVENC/NVDEC usage).</li> <li>These limitations are recognized as technical debt, not functional failures.</li> </ul>"},{"location":"es/1_overview/","title":"Resumen","text":"<p>Esta secci\u00f3n describe la naturaleza del Stream Inference Engine, su posici\u00f3n dentro de la arquitectura global y las responsabilidades que asume en el procesamiento de streams. Define los l\u00edmites funcionales del sistema, su modelo de integraci\u00f3n con otros servicios, la gesti\u00f3n de configuraciones y estado, as\u00ed como el comportamiento ante errores. Finalmente, contextualiza el estado actual como MVP y las l\u00edneas de evoluci\u00f3n orientadas a escalabilidad y mejora de observabilidad.</p>"},{"location":"es/1_overview/#naturaleza-del-sistema","title":"Naturaleza del sistema","text":"<ul> <li>El sistema funciona como un motor de inferencia que ejecuta runtimes de procesamiento de video.</li> <li>Ejecuta exclusivamente reglas definidas por el negocio.</li> <li>No toma decisiones aut\u00f3nomas ni aplica l\u00f3gica de negocio propia.</li> <li>Brinda flexibilidad para inferencia personalizada por cliente.</li> </ul>"},{"location":"es/1_overview/#rol-dentro-de-la-arquitectura","title":"Rol dentro de la arquitectura","text":"<ul> <li>Es parte de una arquitectura de microservicios m\u00e1s amplia.</li> <li>Implementa tres m\u00f3dulos l\u00f3gicos principales:</li> <li>M\u00f3dulo de inferencia</li> <li>M\u00f3dulo de configuraciones y modelos</li> <li>M\u00f3dulo de dibujado</li> <li>Estos m\u00f3dulos se integran en un orquestador denominado <code>inference_engine</code>.</li> <li>El engine obtiene los trabajos a ejecutar mediante polling en una base de datos.</li> <li>La API act\u00faa como intermediario entre otros servicios y el engine.</li> </ul>"},{"location":"es/1_overview/#responsabilidades-principales","title":"Responsabilidades principales","text":"<ul> <li>Ejecutar inferencia sobre streams de video RTSP.</li> <li>Aplicar reglas previamente definidas sobre los resultados de inferencia.</li> <li>Dibujar los resultados directamente sobre los frames.</li> <li>Publicar el stream resultante mediante RTSP.</li> <li>Mantener la ejecuci\u00f3n de los streams independientemente de la existencia de consumidores.</li> </ul>"},{"location":"es/1_overview/#gestion-de-configuraciones-y-modelos","title":"Gesti\u00f3n de configuraciones y modelos","text":"<ul> <li>Las configuraciones y pesos de modelos son administrados por la API.</li> <li>El engine consume \u00fanicamente configuraciones y recursos previamente cargados.</li> <li>Permite inferencia personalizada a partir de configuraciones espec\u00edficas por cliente.</li> </ul>"},{"location":"es/1_overview/#manejo-de-estado-y-persistencia","title":"Manejo de estado y persistencia","text":"<ul> <li>Persiste estado de corto plazo necesario para ejecutar reglas entre frames.</li> <li>El estado se utiliza para detectar eventos dependientes de historial.</li> <li>Los eventos pueden derivar en dibujado visual o notificaci\u00f3n futura.</li> <li>No se persiste informaci\u00f3n hist\u00f3rica de inferencia a largo plazo.</li> </ul>"},{"location":"es/1_overview/#exposicion-e-integracion","title":"Exposici\u00f3n e integraci\u00f3n","text":"<ul> <li>La autenticaci\u00f3n y autorizaci\u00f3n est\u00e1n fuera de scope.</li> <li>El acceso est\u00e1 delegado a otras capas de la arquitectura.</li> <li>El sistema expone streams RTSP sin asumir la existencia de consumidores activos.</li> <li>Otros servicios pueden consumir streams o eventos de forma independiente.</li> </ul>"},{"location":"es/1_overview/#manejo-de-errores-y-ejecucion-de-runs","title":"Manejo de errores y ejecuci\u00f3n de runs","text":"<ul> <li>Si una fuente no es alcanzable inicialmente, el run se marca como fallido.</li> <li>Ante p\u00e9rdida de conexi\u00f3n, se aplican reintentos configurables.</li> <li>Si los reintentos fallan, el run se marca como fallido.</li> <li>Los errores de implementaci\u00f3n se consideran no recuperables.</li> <li>El estado de los runs se persiste en la base de datos.</li> </ul>"},{"location":"es/1_overview/#estado-actual-del-sistema","title":"Estado actual del sistema","text":"<ul> <li>El sistema se encuentra en estado MVP.</li> <li>El foco principal fue la validaci\u00f3n de viabilidad t\u00e9cnica y de negocio.</li> <li>Se realizaron pruebas de estr\u00e9s para identificar l\u00edmites de rendimiento.</li> <li>El sistema presenta margen de mejora con desarrollo adicional.</li> </ul>"},{"location":"es/1_overview/#evolucion-prevista","title":"Evoluci\u00f3n prevista","text":"<ul> <li>La evoluci\u00f3n prioriza escalabilidad del sistema.</li> <li>La escalabilidad impactar\u00e1 en throughput y latencia.</li> <li>La medici\u00f3n precisa de m\u00e9tricas es actualmente limitada.</li> <li>La arquitectura de dos pipelines afecta la trazabilidad de los frames.</li> </ul>"},{"location":"es/2_architecture/","title":"Arquitectura","text":"<p>Esta secci\u00f3n describe la descomposici\u00f3n estructural del sistema en m\u00f3dulos conceptuales, el flujo de datos y control entre ellos, y los mecanismos de comunicaci\u00f3n utilizados. Define las dependencias externas cr\u00edticas (hardware, drivers y frameworks), los l\u00edmites actuales de escalabilidad bajo el hardware objetivo y las m\u00e9tricas consideradas para evaluar comportamiento. Tambi\u00e9n delimita el grado de extensibilidad de los componentes y el modelo de ownership dentro del alcance del MVP.</p>"},{"location":"es/2_architecture/#modulos-conceptuales","title":"M\u00f3dulos conceptuales","text":"<p>El sistema se organiza en dos planos: control y datos.</p> <p>Plano de control - API: Interfaz hacia otros servicios. Gestiona configuraciones, pesos de modelos y persistencia en base de datos. - Engine: Orquestador central. Gestiona el ciclo de vida de los runs mediante polling en la base de datos y coordina Workers y Stream Manager v\u00eda IPC. - Base de datos: Mediador de estado entre API y Engine.</p> <p>Plano de datos - Workers (0..N): Procesos independientes, uno por stream activo. Cada Worker ejecuta un Inference Pipeline que ingesta la fuente RTSP, aplica inferencia y dibujado sobre los frames, los codifica en H264 y los escribe en memoria compartida. - Shared Memory: Mecanismo de transferencia de frames entre Workers y Stream Manager. - Stream Manager: Proceso central del plano de datos. Contiene el Factory Pipeline, que lee frames desde memoria compartida y los publica a trav\u00e9s del servidor RTSP.</p> <pre><code>flowchart TB\n    subgraph CP[\"Plano de control\"]\n        direction TB\n        API[\"API REST\"] &lt;--&gt; DB[(SQLite)] &lt;--&gt; ENG[\"Engine &lt;br/&gt; (Lifecycle \u00b7 Polling \u00b7 IPC)\"]\n    end\n\n    subgraph DP[\"Plano de datos\"]\n        direction LR\n        SRC[\"Fuentes RTSP &lt;br/&gt; (0..N)\"]\n        IP[\"Workers (0..N) &lt;br/&gt; Inference Pipeline &lt;br/&gt; (GStreamer + DeepStream) &lt;br/&gt; Ingesti\u00f3n \u00b7 Inferencia &lt;br/&gt; Dibujado \u00b7 Encode\"]\n        SHM[/\"Shared Memory\"/]\n        FP[\"Stream Manager &lt;br/&gt; Factory Pipeline &lt;br/&gt; RTSP Server\"]\n        OUT[\"Consumidores RTSP &lt;br/&gt; (0..N)\"]\n        SRC --&gt; IP\n        IP --&gt;|\"frames H264\"| SHM --&gt; FP --&gt; OUT\n    end\n\n    ENG -.-&gt; DP\n\n    classDef control fill:#dbeafe,stroke:#3b82f6,color:#1e3a5f\n    classDef data fill:#dcfce7,stroke:#22c55e,color:#14532d\n    classDef external fill:#fef9c3,stroke:#eab308,color:#422006\n    classDef memory fill:#fce7f3,stroke:#ec4899,color:#500724\n\n    class API,DB,ENG control\n    class IP,FP data\n    class SRC,OUT external\n    class SHM memory</code></pre>"},{"location":"es/2_architecture/#flujo-de-control-y-comunicacion","title":"Flujo de control y comunicaci\u00f3n","text":"<ul> <li>API y Engine se comunican exclusivamente a trav\u00e9s de la base de datos.</li> <li>Engine coordina Workers y Stream Manager mediante IPC (stdin/stdout) con mensajes JSON.</li> <li>Los frames procesados se transfieren entre Workers y Stream Manager a trav\u00e9s de memoria compartida.</li> <li>Todos los componentes deben estar operativos para ejecutar un run; si alguno falla, el run no se completa.</li> </ul>"},{"location":"es/2_architecture/#dependencias-externas-y-hardware","title":"Dependencias externas y hardware","text":"<ul> <li>Dependencia fuerte de arquitectura GPU: Tesla V100, NVIDIA T4, Jetson.</li> <li>Librer\u00edas y frameworks requeridos: CUDA 12.6, cuDNN, DeepStream 7.1, pyds, GStreamer plugins, drivers NVIDIA espec\u00edficos, YOLO, PyTorch, TensorRT.</li> <li>API y Engine dependen de la base de datos (actualmente local).</li> <li>M\u00f3dulos dependen del hardware disponible para cumplir con rendimiento esperado.</li> </ul>"},{"location":"es/2_architecture/#escalabilidad-y-limites-conceptuales","title":"Escalabilidad y l\u00edmites conceptuales","text":"<ul> <li>L\u00edmite actual: 6 streams 1080p con inferencia full-frame usando modelo YOLO nano.</li> <li>Potencial estimado: ~10 streams con optimizaci\u00f3n de recursos y sin escalar hardware.</li> <li>Consumo actual para 6 streams: 16GB RAM, &lt;12GB VRAM, GPU Tesla V100, CPU 6 n\u00facleos.</li> <li>Escalar mediante m\u00e1s hardware es m\u00e1s r\u00e1pido pero costoso; optimizaci\u00f3n de c\u00f3digo puede mejorar eficiencia sin aumentar recursos.</li> </ul>"},{"location":"es/2_architecture/#metricas-conceptuales","title":"M\u00e9tricas conceptuales","text":"<ul> <li>Se mide framedrop rate, latencia end-to-end y latencia de procesamiento.</li> <li>El Factory Pipeline mide solo latencia end-to-end; el framedrop no es representativo dado que re-consume los mismos frames desde memoria compartida.</li> <li>La medici\u00f3n real es limitada debido a la p\u00e9rdida de identidad de frames entre pipelines.</li> <li>Futuras mejoras: agregar metadata por frame para trazabilidad entre pipelines y medici\u00f3n precisa de throughput y latencia.</li> </ul>"},{"location":"es/2_architecture/#extensibilidad-y-ownership","title":"Extensibilidad y ownership","text":"<ul> <li>M\u00f3dulos como validaci\u00f3n de reglas, evaluaci\u00f3n de reglas, carga de modelos y dibujado son extensibles, aunque requieren intervenci\u00f3n de un desarrollador.</li> <li>Engine orquesta todos los m\u00f3dulos; API desacopla casi totalmente la interacci\u00f3n mediante la base de datos.</li> <li>Dise\u00f1o conceptual permite escalar y modificar funcionalidades sin afectar otros m\u00f3dulos, respetando l\u00edmites del MVP.</li> </ul>"},{"location":"es/3_performance/","title":"Rendimiento","text":"<p>Esta secci\u00f3n describe el modelo de evaluaci\u00f3n de rendimiento del sistema en el contexto del MVP. Define el entorno de validez, los criterios formales de \u201creal-time\u201d, las m\u00e9tricas utilizadas y sus limitaciones. No constituye un benchmark comparativo ni expone capacidad m\u00e1xima del sistema.</p>"},{"location":"es/3_performance/#alcance-y-contexto-de-validez","title":"Alcance y contexto de validez","text":"<p>Las consideraciones de este documento son v\u00e1lidas \u00fanicamente bajo un entorno controlado equivalente al utilizado durante el desarrollo del MVP:</p> <ul> <li>Arquitectura x86_64  </li> <li>GPU NVIDIA clase datacenter  </li> <li>CPU multin\u00facleo  </li> <li>CUDA + TensorRT + DeepStream compatibles  </li> <li>Streams homog\u00e9neos  </li> <li>Resoluciones entre 480p y 1080p  </li> <li>FPS de entrada en rango 25\u201330  </li> </ul> <p>Cualquier variaci\u00f3n en hardware, drivers, modelos o configuraci\u00f3n invalida la extrapolaci\u00f3n directa del comportamiento descrito.</p>"},{"location":"es/3_performance/#definicion-de-carga","title":"Definici\u00f3n de carga","text":"<p>Se entiende por carga la cantidad de streams procesados en paralelo, cada uno asociado a uno o m\u00e1s modelos de inferencia.</p> <p>El incremento de carga impacta en:</p> <ul> <li>FPS efectivo por stream  </li> <li>Frame drop rate agregado  </li> <li>Latencias promedio  </li> <li>Utilizaci\u00f3n de GPU y memoria  </li> </ul> <p>El sistema presenta degradaci\u00f3n progresiva cuando la contenci\u00f3n de recursos impide sostener el procesamiento en tiempo real para todos los streams activos.</p>"},{"location":"es/3_performance/#criterios-de-aceptacion-real-time","title":"Criterios de aceptaci\u00f3n (Real-Time)","text":"<p>El estado del sistema se clasifica seg\u00fan el frame drop rate agregado:</p> <ul> <li>0% \u2013 1%   Tiempo real \u00f3ptimo dentro del alcance del MVP.</li> <li>1% \u2013 5%   Rendimiento aceptable para validaci\u00f3n funcional.</li> <li>&gt; 5%   El servicio deja de considerarse real-time en el contexto del MVP.</li> </ul> <p>Estos umbrales est\u00e1n definidos con fines de validaci\u00f3n t\u00e9cnica interna y no representan SLA productivos.</p> <p>El estado real-time del sistema se determina utilizando el valor agregado correspondiente a <code>workers_frame_drop_rate_avg</code>.</p>"},{"location":"es/3_performance/#metodologia-de-medicion-alto-nivel","title":"Metodolog\u00eda de medici\u00f3n (alto nivel)","text":"<ul> <li>Las m\u00e9tricas excluyen warm-up y creaci\u00f3n de pipelines.</li> <li>Se capturan \u00fanicamente en r\u00e9gimen estable.</li> <li>Se asume homogeneidad entre streams.</li> <li>Las m\u00e9tricas por stream se promedian temporalmente.</li> <li>Las m\u00e9tricas del sistema son agregaciones peri\u00f3dicas de todas las m\u00e9tricas por stream.</li> <li>No se realiza detecci\u00f3n ni eliminaci\u00f3n de outliers.</li> </ul> <p>El objetivo es caracterizar comportamiento general bajo carga, no realizar an\u00e1lisis estad\u00edstico exhaustivo.</p>"},{"location":"es/3_performance/#metricas-utilizadas","title":"M\u00e9tricas utilizadas","text":"<p>Las m\u00e9tricas utilizadas son agregados persistidos por el Engine y derivados del modelo <code>system_metrics</code>.</p> <p>Se consideran:</p>"},{"location":"es/3_performance/#recursos-del-sistema","title":"Recursos del sistema","text":"<ul> <li><code>gpu_compute_percent</code></li> <li><code>gpu_memory_percent</code></li> <li><code>ram_percent</code></li> <li><code>cpu_percent</code></li> </ul> <p>El frame drop rate es la m\u00e9trica principal de evaluaci\u00f3n, ya que representa de forma directa la incapacidad del sistema de sostener el flujo de frames en tiempo real.</p>"},{"location":"es/3_performance/#comportamiento-bajo-incremento-de-carga","title":"Comportamiento bajo incremento de carga","text":"<p>A medida que aumenta la cantidad de streams en paralelo:</p> <ul> <li>El uso de GPU tiende a crecer hasta aproximarse al punto de saturaci\u00f3n.</li> <li>El frame drop rate aumenta de forma no lineal.</li> <li>La latencia end-to-end promedio se incrementa por contenci\u00f3n de recursos.</li> <li>El FPS efectivo por stream puede degradarse progresivamente.</li> </ul> <p>El punto de degradaci\u00f3n no es una propiedad te\u00f3rica del hardware, sino una consecuencia del estado actual de la implementaci\u00f3n.</p> <p>No se publica en este documento el l\u00edmite cuantitativo de streams sostenibles.</p>"},{"location":"es/3_performance/#limitaciones-de-medicion","title":"Limitaciones de medici\u00f3n","text":"<ul> <li>No se mide jitter.</li> <li>No se mide fairness entre streams.</li> <li>No se calcula latencia end-to-end real con trazabilidad de frame completa.</li> <li>No se eval\u00faa precisi\u00f3n ni calidad de inferencia.</li> <li>No se realiza an\u00e1lisis estad\u00edstico profundo.</li> </ul> <p>La p\u00e9rdida de identidad de frame entre pipelines impide el c\u00e1lculo exacto de latencia end-to-end real.</p>"},{"location":"es/3_performance/#implicancias-y-evolucion-esperada","title":"Implicancias y evoluci\u00f3n esperada","text":"<p>El l\u00edmite operativo actual del MVP est\u00e1 determinado principalmente por eficiencia de implementaci\u00f3n y uso de recursos, no por restricciones estructurales de arquitectura.</p> <p>Mejoras en:</p> <ul> <li>Gesti\u00f3n de memoria</li> <li>Reducci\u00f3n de copias innecesarias</li> <li>Control din\u00e1mico de carga</li> <li>Instrumentaci\u00f3n de m\u00e9tricas m\u00e1s precisa  </li> </ul> <p>deber\u00edan permitir aumentar la capacidad efectiva manteniendo los umbrales definidos de real-time.</p> <p>La evoluci\u00f3n futura prioriza escalabilidad horizontal por stream y control expl\u00edcito de degradaci\u00f3n.</p>"},{"location":"es/3_performance/#fuera-de-alcance","title":"Fuera de alcance","text":"<p>Este cap\u00edtulo no cubre:</p> <ul> <li>Capacidad m\u00e1xima exacta del sistema</li> <li>Procedimientos detallados de benchmarking</li> <li>Comparativas con otras implementaciones</li> <li>SLA productivos</li> <li>Escalabilidad multinodo</li> </ul>"},{"location":"es/4_engineering-decisions/","title":"Decisiones de dise\u00f1o","text":"<p>Este cap\u00edtulo documenta las decisiones t\u00e9cnicas estructurales que definieron la arquitectura actual del sistema, incluyendo motivaciones, restricciones del entorno, alternativas descartadas y costos asumidos. Expone c\u00f3mo las prioridades del MVP \u2014estabilidad, viabilidad funcional y extensibilidad\u2014 condicionaron elecciones en separaci\u00f3n de procesos, uso de GPU, gesti\u00f3n de memoria, m\u00e9tricas y desacoplamiento entre componentes, as\u00ed como las implicancias t\u00e9cnicas que estas decisiones generan para la evoluci\u00f3n futura del engine.</p>"},{"location":"es/4_engineering-decisions/#1-objetivos-tecnicos-y-prioridades","title":"1. Objetivos t\u00e9cnicos y prioridades","text":"<ul> <li>Objetivos primarios</li> <li>Best-effort zero-copy a lo largo del pipeline.</li> <li>Alta extensibilidad en cantidad de modelos, pesos y tareas de inferencia.</li> <li>Mantener siempre una salida en tiempo real (real-time output).</li> <li>Priorizaci\u00f3n</li> <li>Se prioriz\u00f3 viabilidad funcional sobre performance absoluta.</li> <li>El sistema se concibi\u00f3 como un MVP orientado a validar factibilidad t\u00e9cnica y del modelo de negocios.</li> <li>Objetivos descartados o postergados</li> <li>Zero-copy total extremo a extremo.</li> <li>Obtenci\u00f3n de m\u00e9tricas de rendimiento completamente exactas.</li> <li>Soporte amplio de frameworks de inferencia desde el inicio.</li> <li>Alcance inicial de frameworks</li> <li>Enfoque principal en YOLO.</li> <li>Modelos espec\u00edficos como anomalib fueron expl\u00edcitamente postergados.</li> </ul>"},{"location":"es/4_engineering-decisions/#2-separacion-en-procesos-y-pipelines","title":"2. Separaci\u00f3n en procesos y pipelines","text":"<ul> <li>Motivaci\u00f3n principal</li> <li>Incompatibilidades cr\u00edticas entre CUDA, librer\u00edas de procesamiento en Python, GStreamer, PyGObject y DeepStream dentro de un mismo runtime.</li> <li>Problemas observados</li> <li>Ca\u00eddas del proceso Python sin captura de excepciones.</li> <li>Fallos al:<ul> <li>Inicializar pipelines de GStreamer.</li> <li>Acceder a GstBuffer desde transforms.</li> <li>Iniciar el debugger de Python con GStreamer activo.</li> <li>Crear m\u00faltiples pipelines en un mismo runtime.</li> <li>Usar simult\u00e1neamente encoder y decoder por hardware.</li> </ul> </li> <li>Hip\u00f3tesis t\u00e9cnica</li> <li>Librer\u00edas CUDA no reentrantes.</li> <li>Corrupci\u00f3n de estado interno de memoria al cargarlas m\u00faltiples veces en un mismo runtime.</li> <li>Decisi\u00f3n</li> <li>Separaci\u00f3n estricta en procesos independientes usando <code>subprocess</code>.</li> <li>Topolog\u00eda resultante</li> </ul> <pre><code>Inference Engine\n\u251c\u2500 Stream Manager Bridge (IPC)\n\u251c\u2500 Stream Manager\n\u2502  \u251c\u2500 RTSP Server\n\u2502  \u251c\u2500 Factory Pipeline\n\u2502  \u2514\u2500 Workers (0..N)\n\u2514\u2500 Stream Workers\n   \u2514\u2500 Inference Pipeline (0..N)\n</code></pre> <ul> <li>Comunicaci\u00f3n basada en IPC (stdin/stdout) y memoria compartida.</li> <li>Separaci\u00f3n Factory Pipeline / Inference Pipeline</li> <li>Factory Pipeline (Stream Manager): lee desde memoria compartida (shmsrc) y alimenta el servidor RTSP.</li> <li>Inference Pipeline (Workers): ingesti\u00f3n RTSP \u2192 inferencia \u2192 dibujado \u2192 encode H264 \u2192 memoria compartida (shmsink).</li> <li>Motivaci\u00f3n adicional</li> <li>Incompatibilidad entre runtime de servidor RTSP y librer\u00edas DeepStream.</li> <li>Aislamiento del runtime evita colisiones de dependencias (incluyendo versiones previas como Savant).</li> <li>Costos aceptados</li> <li>Mayor latencia.</li> <li>Mayor tiempo de inicializaci\u00f3n.</li> <li>Mayor complejidad operativa.</li> <li>Beneficio</li> <li>Estabilidad del sistema y ejecuci\u00f3n consistente.</li> </ul>"},{"location":"es/4_engineering-decisions/#3-eleccion-de-gstreamer-deepstream-y-frameworks","title":"3. Elecci\u00f3n de GStreamer, DeepStream y frameworks","text":"<ul> <li>Origen de la decisi\u00f3n</li> <li>GStreamer fue un requisito impuesto externamente.</li> <li>Implementaci\u00f3n inicial descartada</li> <li>OpenCV + NumPy en CPU: viable funcionalmente, inviable para GPU zero-copy.</li> <li>Motivaci\u00f3n del cambio</li> <li>Necesidad de consumir RTSP y generar frames directamente en GPU.</li> <li>Stack seleccionado</li> <li>GStreamer + DeepStream para ingesti\u00f3n y procesamiento GPU-native.</li> <li>TensorRT como parte del runtime NVIDIA (disponible a trav\u00e9s del SDK de DeepStream y CUDA Toolkit, no como dependencia declarada del proyecto).</li> <li>Frameworks de inferencia</li> <li>YOLO como primer framework por:</li> <li>Abundancia de modelos.</li> <li>Enfoque Python-first.</li> <li>Alta extensibilidad sin modificar el core del sistema.</li> <li>Protocolo</li> <li>RTSP impuesto por las fuentes externas (limitaci\u00f3n del dominio del problema).</li> </ul>"},{"location":"es/4_engineering-decisions/#4-gestion-de-memoria-y-frames","title":"4. Gesti\u00f3n de memoria y frames","text":"<ul> <li>Principio rector</li> <li>Minimizar copias de frames (CPU\u2194GPU y GPU\u2194GPU).</li> <li>Beneficios</li> <li>Menor consumo de RAM y VRAM.</li> <li>Mejor throughput.</li> <li>Menor frame drop rate.</li> <li>Decisi\u00f3n cr\u00edtica</li> <li>Aceptar la p\u00e9rdida de identidad del frame entre pipelines.</li> <li>Justificaci\u00f3n</li> <li>Preservar identidad requiere:</li> <li>Allocaci\u00f3n adicional de memoria compartida.</li> <li>Header de metadata persistente por frame.</li> <li>Alto esfuerzo de desarrollo no compatible con tiempos del MVP.</li> <li>Consecuencia actual</li> <li>El Inference Pipeline genera frames procesados a ~25 FPS en memoria compartida.</li> <li>El Factory Pipeline consume memoria compartida a ~45 FPS, retransmitiendo el mismo frame varias veces hacia el servidor RTSP.</li> <li>Decisiones t\u00e9cnicas espec\u00edficas</li> <li>Uso de DeepStream para generar frames directamente en GPU.</li> <li>Uso de CuPy para capturar referencias GPU en Python.</li> <li>Uso de PyTorch por su abstracci\u00f3n de tensores desacoplada del dispositivo.</li> <li>Dibujado directo sobre frame siempre que es posible.</li> <li>Uso de metadata + <code>nvdsosd</code> para texto y overlays en GPU.</li> </ul>"},{"location":"es/4_engineering-decisions/#5-concurrencia-carga-y-frame-drop","title":"5. Concurrencia, carga y frame drop","text":"<ul> <li>L\u00edmite de streams</li> <li>No impuesto expl\u00edcitamente por c\u00f3digo.</li> <li>Determinado emp\u00edricamente por consumo de recursos.</li> <li>Frame drop</li> <li>Aceptado como m\u00e9trica principal.</li> <li>Es la m\u00e9trica m\u00e1s fiel dadas las limitaciones actuales.</li> <li>Backpressure</li> <li>Implementado mediante colas que descartan frames antes del procesamiento.</li> <li>M\u00e9tricas</li> <li>FPS de salida suficiente para inferir frame drop rate.</li> <li>Control de carga</li> <li>No implementado.</li> <li>De existir, estar\u00eda basado en consumo de recursos, no en m\u00e9tricas de rendimiento.</li> </ul>"},{"location":"es/4_engineering-decisions/#6-api-base-de-datos-y-desacoplamiento","title":"6. API, base de datos y desacoplamiento","text":"<ul> <li>Decisi\u00f3n</li> <li>Desacoplar API e Inference Engine mediante base de datos.</li> <li>Motivaci\u00f3n</li> <li>Evitar sincronizaci\u00f3n de estado entre runtimes heterog\u00e9neos.</li> <li>Permitir m\u00faltiples instancias de inference engine en el futuro.</li> <li>Estado actual</li> <li>Base de datos local.</li> <li>Soporte futuro para escalado horizontal no implementado.</li> </ul>"},{"location":"es/4_engineering-decisions/#7-observabilidad-postergado","title":"7. Observabilidad (postergado)","text":"<ul> <li>Estado</li> <li>Parcialmente abordado.</li> <li>Limitado por p\u00e9rdida de identidad de frame.</li> <li>Deuda t\u00e9cnica</li> <li>M\u00e9tricas precisas de latencia por frame.</li> <li>Trazabilidad completa del pipeline.</li> </ul>"},{"location":"es/4_engineering-decisions/#8-decisiones-de-alcance","title":"8. Decisiones de alcance","text":"<ul> <li>Excluido del MVP</li> <li>Control de carga avanzado.</li> <li>M\u00e9tricas exhaustivas.</li> <li>Gesti\u00f3n completa del lifecycle.</li> <li>Razonamiento</li> <li>No cr\u00edticas para validar viabilidad del negocio.</li> </ul>"},{"location":"es/4_engineering-decisions/#9-portabilidad-y-hardware","title":"9. Portabilidad y hardware","text":"<ul> <li>Plataformas soportadas</li> <li>x86: Tesla V100, T4.</li> <li>Jetson: JetPack 6.1.</li> <li>Objetivo</li> <li>Viabilidad tanto en edge como en datacenter.</li> <li>Resultado</li> <li>Requisito cumplido, aunque con deuda t\u00e9cnica pendiente.</li> </ul>"},{"location":"es/4_engineering-decisions/#10-evolucion-futura","title":"10. Evoluci\u00f3n futura","text":"<ul> <li>Decisiones condicionantes</li> <li>Arquitectura de pipelines desacoplados con memoria compartida.</li> <li>Componentes reemplazables</li> <li>Frameworks de inferencia.</li> <li>Modelos y pesos.</li> <li>Pendiente de revisi\u00f3n</li> <li>Optimizaci\u00f3n de performance.</li> <li>Compatibilidad de librer\u00edas.</li> <li>Observabilidad y m\u00e9tricas.</li> </ul>"},{"location":"es/5_limitations/","title":"Limitaciones","text":"<p>Esta secci\u00f3n explicita las restricciones t\u00e9cnicas observadas en el estado actual del MVP, diferenciando l\u00edmites derivados de implementaci\u00f3n, entorno y prioridades de alcance respecto de limitaciones estructurales del dise\u00f1o. Define los bordes operativos actuales del sistema en t\u00e9rminos de concurrencia, zero-copy, m\u00e9tricas, escalabilidad y observabilidad, estableciendo qu\u00e9 comportamientos son inherentes al estado presente y cu\u00e1les constituyen deuda t\u00e9cnica planificada para evoluci\u00f3n futura.</p>"},{"location":"es/5_limitations/#limite-de-concurrencia-de-streams-mvp","title":"L\u00edmite de Concurrencia de Streams (MVP)","text":"<ul> <li>El MVP soporta actualmente hasta 6 streams concurrentes con rendimiento aceptable.</li> <li>Este l\u00edmite surge de consumo agregado de recursos (RAM, VRAM, encoder/decoder por hardware y c\u00f3mputo GPU), no de una restricci\u00f3n l\u00f3gica del sistema.</li> <li>El l\u00edmite no est\u00e1 impuesto por configuraci\u00f3n ni hardcoded; es el punto a partir del cual el frame drop rate supera los umbrales aceptables de real-time.</li> <li>Con optimizaci\u00f3n t\u00e9cnica adicional, se estima que el sistema podr\u00eda escalar hasta ~10 streams sobre el mismo hardware, dependiendo de:</li> <li>resoluci\u00f3n,</li> <li>FPS de entrada,</li> <li>complejidad del modelo,</li> <li>tipo de tarea de inferencia.</li> <li>Aun con optimizaci\u00f3n, el hardware impone un techo pr\u00e1ctico que limita la escalabilidad sin aumento de recursos.</li> </ul>"},{"location":"es/5_limitations/#limitaciones-de-zero-copy","title":"Limitaciones de Zero-Copy","text":"<ul> <li>No se ha logrado un esquema de zero-copy completo a lo largo de todo el pipeline.</li> <li>En plataformas Jetson, aunque RAM y VRAM comparten el mismo chip, el zero-copy total no es viable dentro de los l\u00edmites actuales de:</li> <li>Python,</li> <li>CUDA,</li> <li>PyTorch,</li> <li>DeepStream,</li> <li>GStreamer.</li> <li>Durante la fase de dibujado:</li> <li>ciertas operaciones (por ejemplo, segmentaci\u00f3n) resultan m\u00e1s simples de implementar en CPU usando OpenCV,</li> <li>lo que introduce copias expl\u00edcitas o impl\u00edcitas de memoria.</li> <li>Algunos modelos requieren entrada en formato <code>float32</code>, lo que implica conversiones y copias adicionales.</li> <li>En la pr\u00e1ctica, los frameworks de inferencia realizan copias internas, por lo que este comportamiento es esperado.</li> <li>Conclusi\u00f3n: se aplica un enfoque best-effort zero-copy, manteniendo los frames en GPU el mayor tiempo posible, pero aceptando copias cuando son necesarias.</li> </ul>"},{"location":"es/5_limitations/#limitaciones-de-metricas-y-observabilidad","title":"Limitaciones de M\u00e9tricas y Observabilidad","text":"<ul> <li>La arquitectura basada en dos pipelines desacoplados mediante memoria compartida provoca la p\u00e9rdida de identidad de frame entre pipelines.</li> <li>Esto impide medir de forma precisa:</li> <li>latencia end-to-end real,</li> <li>throughput efectivo,</li> <li>correlaci\u00f3n frame-a-frame entre entrada y salida.</li> <li>Actualmente:</li> <li>el frame drop rate se utiliza como m\u00e9trica principal para evaluar comportamiento real-time,</li> <li>otras m\u00e9tricas son aproximadas.</li> <li>Implementar trazabilidad completa requerir\u00eda:</li> <li>extender el esquema de memoria compartida con headers de metadata,</li> <li>o introducir mecanismos expl\u00edcitos de sincronizaci\u00f3n entre procesos.</li> <li>Dado el alcance del MVP, esta mejora se considera deuda t\u00e9cnica.</li> </ul>"},{"location":"es/5_limitations/#limitaciones-de-alcance-del-mvp","title":"Limitaciones de Alcance del MVP","text":"<ul> <li>El sistema se desarroll\u00f3 como MVP orientado a validaci\u00f3n t\u00e9cnica y de negocio, no como producto final.</li> <li>No se implementaron:</li> <li>mecanismos de control de carga basados en recursos,</li> <li>l\u00edmites din\u00e1micos de ejecuci\u00f3n de runs,</li> <li>escalado autom\u00e1tico.</li> <li>El n\u00famero de frameworks de inferencia soportados es limitado:</li> <li>YOLO como framework principal,</li> <li>soporte puntual para Anomalib.</li> <li>La calidad de inferencia depende exclusivamente del modelo cargado; el sistema no aplica validaci\u00f3n sem\u00e1ntica de resultados.</li> <li>El sistema no garantiza:</li> <li>accesibilidad de las fuentes,</li> <li>estabilidad de la red,</li> <li>calidad de imagen de entrada,</li> <li>comportamiento de los consumidores de los streams.</li> </ul>"},{"location":"es/5_limitations/#deuda-tecnica-resuelta","title":"Deuda T\u00e9cnica Resuelta","text":"<ul> <li>Se elimin\u00f3 la cache de frames completos en CPU del m\u00f3dulo de dibujado, reduciendo el consumo excesivo de RAM observado en versiones anteriores. El m\u00f3dulo mantiene cache de resultados de inferencia y metadata de dibujado entre frames, pero ya no almacena copias de frames en memoria CPU.</li> </ul>"},{"location":"es/5_limitations/#mejoras-diferidas","title":"Mejoras Diferidas","text":"<p>Las siguientes limitaciones se consideran abordables en fases futuras de desarrollo:</p> <ul> <li>Optimizaci\u00f3n adicional del consumo de memoria RAM y VRAM.</li> <li>Mejora del lifecycle de runs y mayor resiliencia ante fallos.</li> <li>Implementaci\u00f3n de control de carga basado en m\u00e9tricas de recursos.</li> <li>Mejora de m\u00e9tricas y trazabilidad de frames.</li> <li>Ampliaci\u00f3n del soporte de frameworks y tipos de modelos.</li> </ul>"},{"location":"es/6_production/","title":"Producci\u00f3n (MVP)","text":"<p>Esta secci\u00f3n delimita el comportamiento operativo del sistema en su estado actual bajo un entorno productivo controlado de tipo single-node. Define supuestos de ejecuci\u00f3n, restricciones de despliegue, l\u00edmites de resiliencia y ausencia de automatizaci\u00f3n avanzada, dejando expl\u00edcito qu\u00e9 garant\u00edas m\u00ednimas existen y qu\u00e9 aspectos permanecen como deuda t\u00e9cnica. No constituye un dise\u00f1o definitivo de producci\u00f3n ni establece compromisos formales de disponibilidad o SLA.</p>"},{"location":"es/6_production/#1-scope-de-produccion","title":"1. Scope de Producci\u00f3n","text":"<ul> <li>Arquitectura single-node.</li> <li>Se asume 1 GPU por host.</li> <li>No se contempla alta disponibilidad (HA) ni despliegue multi-node.</li> <li>La escalabilidad horizontal queda fuera de scope del MVP.</li> <li>El l\u00edmite pr\u00e1ctico de streams est\u00e1 condicionado tanto por consumo de recursos como por restricciones arquitect\u00f3nicas del pipeline.</li> </ul>"},{"location":"es/6_production/#2-arquitectura-de-ejecucion","title":"2. Arquitectura de Ejecuci\u00f3n","text":""},{"location":"es/6_production/#21-ciclo-de-vida-del-engine","title":"2.1 Ciclo de vida del Engine","text":"<ul> <li>El engine se ejecuta de forma persistente y no finaliza por inactividad.</li> <li>Permanece en espera indefinida de solicitudes de <code>run</code>.</li> <li>Si ninguna fuente conecta durante la inicializaci\u00f3n, el sistema no se considera desplegado funcionalmente y queda en estado de espera.</li> <li>Se asume que, en ese escenario, los <code>run</code> ser\u00e1n solicitados nuevamente de forma externa.</li> <li>Este comportamiento es susceptible de cambio en futuras iteraciones.</li> </ul>"},{"location":"es/6_production/#22-estrategia-de-reintentos","title":"2.2 Estrategia de Reintentos","text":"<ul> <li>Se utiliza una pol\u00edtica de retry fijo.</li> <li>No existe backoff exponencial.</li> <li>El sistema se considera UP aunque solo una fuente haya conectado correctamente.</li> <li>Esta l\u00f3gica constituye uno de los puntos m\u00e1s d\u00e9biles del MVP y requiere ingenier\u00eda adicional para producci\u00f3n real.</li> </ul>"},{"location":"es/6_production/#3-despliegue","title":"3. Despliegue","text":""},{"location":"es/6_production/#31-targets-soportados","title":"3.1 Targets Soportados","text":"<ul> <li><code>x86_64</code> con GPU.</li> <li><code>arm64</code> (Jetson).</li> </ul>"},{"location":"es/6_production/#32-proceso-de-build","title":"3.2 Proceso de Build","text":"<ul> <li> <p>El build de im\u00e1genes se realiza en hosts de desarrollo:</p> </li> <li> <p>Host x86 para im\u00e1genes <code>x86_64</code>.</p> </li> <li>Jetson de desarrollo para im\u00e1genes <code>arm64</code>.</li> <li>No existe un pipeline CI/CD automatizado.</li> </ul>"},{"location":"es/6_production/#33-distribucion-y-activacion","title":"3.3 Distribuci\u00f3n y Activaci\u00f3n","text":"<ul> <li>Las im\u00e1genes se transfieren manualmente mediante scripts basados en <code>rsync</code>.</li> <li>El despliegue implica detener y reiniciar manualmente los servicios.</li> <li>Se asume un downtime aproximado de 10 minutos.</li> <li>Este mecanismo no es deseable para el producto final y ser\u00e1 reemplazado en fases posteriores.</li> </ul>"},{"location":"es/6_production/#4-gestion-de-recursos-y-fallos-conocidos","title":"4. Gesti\u00f3n de Recursos y Fallos Conocidos","text":""},{"location":"es/6_production/#41-saturacion-de-recursos","title":"4.1 Saturaci\u00f3n de Recursos","text":"<ul> <li>La saturaci\u00f3n de CPU, GPU o memoria es un escenario esperado.</li> <li>El sistema no implementa mecanismos autom\u00e1ticos de throttling ni degradaci\u00f3n controlada.</li> </ul>"},{"location":"es/6_production/#42-disponibilidad-de-fuentes","title":"4.2 Disponibilidad de Fuentes","text":"<ul> <li> <p>El sistema puede fallar si:</p> </li> <li> <p>La fuente no est\u00e1 disponible durante la inicializaci\u00f3n.</p> </li> <li>La fuente deja de estar disponible tras haberse conectado al menos una vez.</li> <li>El comportamiento ante estos escenarios no est\u00e1 completamente determinado y forma parte de la deuda t\u00e9cnica del MVP.</li> </ul>"},{"location":"es/6_production/#43-criterios-de-fallo-del-proceso","title":"4.3 Criterios de Fallo del Proceso","text":"<ul> <li>Los criterios formales para terminar el proceso (fatal vs recoverable) no est\u00e1n definidos.</li> <li> <p>Requiere an\u00e1lisis adicional para:</p> </li> <li> <p>OOM en GPU o RAM.</p> </li> <li>Bloqueos del pipeline.</li> <li>Degradaci\u00f3n sostenida (frame drop, latencia).</li> </ul>"},{"location":"es/6_production/#5-pipeline-de-memoria-y-zero-copy","title":"5. Pipeline de Memoria y Zero-Copy","text":"<ul> <li>No se garantiza zero-copy end-to-end.</li> <li> <p>Limitaciones principales:</p> </li> <li> <p>Modificaci\u00f3n de frames en CPU durante la fase de dibujo (OpenCV), especialmente en segmentaci\u00f3n.</p> </li> <li>Modelos que requieren entrada en <code>float32</code>.</li> <li>Copias impl\u00edcitas realizadas por PyTorch durante inferencia.</li> <li>En Jetson, la arquitectura de memoria unificada reduce el costo respecto a x86, pero no elimina las copias.</li> <li>Conclusi\u00f3n: zero-copy solo es posible dentro de los l\u00edmites impuestos por Python + CUDA + PyTorch.</li> </ul>"},{"location":"es/6_production/#6-metricas-y-observabilidad","title":"6. M\u00e9tricas y Observabilidad","text":"<p>Las m\u00e9tricas de producci\u00f3n no est\u00e1n completamente definidas en el MVP.</p> <p>Este documento deja abierto determinar:</p> <ul> <li>Qu\u00e9 m\u00e9tricas ser\u00e1n recolectadas (FPS, frame drop, latencia, uso de recursos).</li> <li>Nivel de granularidad (global vs por stream).</li> <li>Medio de exposici\u00f3n (logs, stdout, endpoint).</li> </ul>"},{"location":"es/6_production/#7-configuracion-y-seguridad","title":"7. Configuraci\u00f3n y Seguridad","text":"<ul> <li>La pol\u00edtica de configuraci\u00f3n (build-time vs runtime) no est\u00e1 definida.</li> <li>Aspectos de seguridad (credenciales, secretos, autenticaci\u00f3n de fuentes) quedan fuera de scope del MVP.</li> </ul>"},{"location":"es/6_production/#8-limitaciones-del-mvp","title":"8. Limitaciones del MVP","text":"<ul> <li>Operaci\u00f3n manual.</li> <li>Downtime aceptado.</li> <li>Sin HA.</li> <li>Sin auto-recovery robusto.</li> <li>Observabilidad limitada.</li> </ul> <p>Estas limitaciones son conocidas y se consideran aceptables \u00fanicamente en el contexto de MVP.</p>"},{"location":"es/7_translation_table/","title":"Tabla de Traducci\u00f3n","text":"<p>Tabla que muestra por termino su significado y equivalente o nombre original en ingl\u00e9s </p> Espa\u00f1ol Ingl\u00e9s definitivo tasa de p\u00e9rdida de frames frame drop rate latencia de procesamiento processing latency latencia extremo a extremo end-to-end latency ejecuci\u00f3n de run run execution plano de control control plane plano de datos data plane memoria compartida shared memory dibujado drawing ingesti\u00f3n ingestion fuentes RTSP RTSP sources consumidores RTSP RTSP consumers pesos de modelos model weights tiempo real real time p\u00e9rdida de identidad de frame loss of frame identity deuda t\u00e9cnica technical debt ciclo de vida lifecycle carga load saturaci\u00f3n de recursos resource saturation rendimiento performance rendimiento aceptable acceptable performance"},{"location":"es/","title":"Motor de Inferencia de streams","text":"<p>Este documento define el alcance funcional y operativo del Stream Inference Engine. Describe el problema que resuelve, las restricciones impuestas por el contexto del MVP, los supuestos t\u00e9cnicos bajo los que opera y los l\u00edmites expl\u00edcitos de responsabilidad del sistema. No detalla implementaci\u00f3n interna ni decisiones de ingenier\u00eda espec\u00edficas, sino el marco contractual y operativo bajo el cual el sistema debe evaluarse.</p>"},{"location":"es/#proposito-del-sistema-problem-space","title":"Prop\u00f3sito del sistema (Problem Space)","text":"<ul> <li>Procesar m\u00faltiples streams de video concurrentes en tiempo real.</li> <li>Ejecutar inferencia sobre streams de video y generar salida visual.</li> <li>Publicar el resultado como stream de video, no como metadata estructurada.</li> <li>Operar en contexto productivo B2B, no como servicio gen\u00e9rico o self-service.</li> <li>Definir \"tiempo real\" mediante frame-drop rate (&lt; 5% respecto al framerate de entrada).</li> </ul>"},{"location":"es/#supuestos-del-contexto-assumptions","title":"Supuestos del contexto (Assumptions)","text":"<ul> <li>Las fuentes de video son c\u00e1maras remotas externas.</li> <li>Las fuentes exponen streams v\u00eda RTSP.</li> <li>El codec de entrada es H.264 (impuesto por el contexto del MVP).</li> <li>El protocolo de salida es RTSP.</li> <li>La inferencia se ejecuta prioritariamente sobre GPU.</li> <li>El sistema se despliega sobre plataformas x86_64 y arm64 (Jetson).</li> <li>Varias decisiones t\u00e9cnicas fueron impuestas por el contexto organizacional del MVP.</li> </ul>"},{"location":"es/#restricciones-no-negociables-constraints","title":"Restricciones no negociables (Constraints)","text":"<ul> <li>El sistema no controla el protocolo ni el codec de las fuentes.</li> <li>El sistema no controla la disponibilidad ni estabilidad de la red.</li> <li>La salida debe mantenerse en tiempo real bajo la m\u00e9trica definida.</li> <li>La salida es exclusivamente video con overlays; no se emite metadata adicional.</li> <li>Se priorizan continuidad, estabilidad y latencia por sobre optimizaci\u00f3n extrema.</li> <li>El servicio se entrega directamente en un contexto B2B (sin elasticidad autom\u00e1tica).</li> </ul>"},{"location":"es/#postura-operativa-del-sistema","title":"Postura operativa del sistema","text":"<ul> <li>El sistema valida la alcanzabilidad de la fuente antes de iniciar el procesamiento.</li> <li>Ante p\u00e9rdida de conexi\u00f3n, el sistema reintenta seg\u00fan configuraci\u00f3n.</li> <li>Si se excede el n\u00famero de reintentos, el stream se considera fallido.</li> <li>En el estado actual del MVP no existen mecanismos autom\u00e1ticos de limitaci\u00f3n de carga.</li> <li>Solo se ofrecen configuraciones consideradas aceptables para no degradar performance.</li> </ul>"},{"location":"es/#limites-explicitos-de-responsabilidad-responsibility-boundaries","title":"L\u00edmites expl\u00edcitos de responsabilidad (Responsibility Boundaries)","text":"<ul> <li>El sistema no garantiza la calidad ni correcci\u00f3n de la inferencia.</li> <li>La accuracy es responsabilidad del modelo cargado.</li> <li>El sistema no garantiza accesibilidad de las fuentes.</li> <li>Fallas de red est\u00e1n fuera de scope.</li> <li>Fallas del emisor del stream est\u00e1n fuera de scope.</li> <li>Fallas de sistemas consumidores de la salida est\u00e1n fuera de scope.</li> <li>El sistema no garantiza optimizaci\u00f3n \u00f3ptima de recursos en el estado actual del MVP.</li> </ul>"},{"location":"es/#estado-actual-del-sistema","title":"Estado actual del sistema","text":"<ul> <li>El sistema se encuentra en estado MVP.</li> <li>Demuestra viabilidad funcional end-to-end:</li> <li>inicio de streams</li> <li>ejecuci\u00f3n de inferencia</li> <li>visualizaci\u00f3n del resultado en un frontend</li> <li>Existen limitaciones conocidas de consumo de recursos (RAM, uso parcial de NVENC/NVDEC).</li> <li>Estas limitaciones se reconocen como deuda t\u00e9cnica y no como fallas funcionales.</li> </ul>"}]}