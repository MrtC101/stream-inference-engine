{"config":{"lang":["en","es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"1_overview/","title":"Overview draft","text":"<p>This project presents a technical overview of a MindColab's new real-time video processing engine designed to use interchangeable AI models to perform inference over multiple continuous video streams. The system, hereafter referred to as inference engine, acts as a core piece for a microservice architecture used to deliver a commercial service called \"ironstream\". To make the engine a reliable solution, the implementation and internal design need to meet key requirements such as 24/7 availability, efficient use of computational resources, state-of-the-art frameworks and extensibility and customization of inference features.</p> <p>The primary goal is not to showcase a finished product, but to document engineering decisions, trade-offs, and real-world limitations encountered while building a functional MVP under practical constraints such as time, hardware, and system complexity.</p> <p>The system evolved from an initial CPU-only implementation\u2014unable to sustain real-time performance\u2014into a GPU-accelerated architecture based on GStreamer, CUDA/NVMM, and decoupled inference components. </p> <p>The first CPU-bound approach took over 3 months of implementation to reach a functional starting point. This implementation included - Project setup using uv  - Development container with cpu-bound libraries  - Configuration yaml field validation using Cerverus - Design of a DSL grammar for Inference Rules and implementation of a interpreter built with lark - DSL to create yaml configurations and templates to inference specifications and streams configuration. - OpenCV based approach to read from stream sources. - A zero-copy Frame transmition using numpy. - Motion detection mechanism to trigger inference over specific frames.</p> <p>The second step was to implement a GPU-bound apporach over the next 4 months.  - Re-implementation of zero-copy approach using pytorch tensors. - Re design  Development container to install GPU specifi frameworks as Cuda, CDNN, Deepstream, Gstreamer - First intent of a production ready docker container - Inference model architecture to add Inference models extensibiliy - Implementing a multiprocessing approch with custom IPC and shared memory approach to transfer frames between piplines using gstreamr's shmsrc. - Implementing a numba kernel to draw lines in a GPU - Integratting drawing module to show visual metada and text using deepstream plugin nvdsosd - Implement and optimize dockerfile and docker compose file to build the lightests posible docker image - Developed simple API to query stream processing jobs called \"runs\" and store the in a sqlite database. - Implemented mechanism to read database runs, update state - Implemente engine metric capture mechanism to evaluate performance . notebook with analisis of system over stress with the inference engien in mvp state.</p>"},{"location":"1_overview/#project-scope","title":"Project Scope","text":"<p>The current scope of the project includes:</p> <ul> <li>Real-time video stream ingestion.</li> <li>GPU-accelerated processing and inference.</li> <li>Re-streaming of processed video with bounded latency.</li> <li>Continuous operation in production-like environments (24/7).</li> <li>Hot-reload support for selected configuration parameters.</li> </ul> <p>Explicitly out of scope at this stage:</p> <ul> <li>Automatic horizontal scaling.</li> <li>Advanced orchestration (Kubernetes, autoscaling).</li> <li>Fully guaranteed end-to-end zero-copy.</li> <li>Dynamic redeploy without process restart.</li> <li>Exhaustive performance analysis and deep profiling.</li> </ul>"},{"location":"1_overview/#design-principles","title":"Design Principles","text":"<p>Engineering decisions throughout the project prioritized:</p> <ul> <li>Operational simplicity over maximum flexibility.</li> <li>Clear separation of responsibilities between pipelines.</li> <li>Minimal yet sufficient observability for an MVP.</li> <li>Incremental evolution instead of premature over-engineering.</li> <li>Explicit use of NVMM buffers to reduce unnecessary memory copies.</li> </ul> <p>The system is designed to be understandable, modifiable, and extensible, even where optimizations are intentionally left unimplemented.</p>"},{"location":"1_overview/#current-state-mvp","title":"Current State (MVP)","text":"<p>In its current state, the system:</p> <ul> <li>Operates in real time under controlled conditions.</li> <li>Measures end-to-end latency in independent pipeline segments.</li> <li>Uses GPU resources consistently, with basic metrics for GPU, VRAM, and RAM utilization.</li> <li>Supports controlled restarts, graceful shutdown, and partial configuration hot-reload.</li> <li>Contains known inefficiencies, particularly in rendering and frame drawing stages.</li> </ul> <p>This state is considered intentionally incomplete, but stable and observable.</p>"},{"location":"1_overview/#target-audience","title":"Target Audience","text":"<p>This documentation is intended for:</p> <ul> <li>Engineers interested in real-time video architectures.</li> <li>Technical reviewers evaluating engineering judgment, not just features.</li> <li>Teams that value explicit acknowledgment of limitations over polished claims.</li> </ul> <p>It is not meant to be end-user documentation or marketing material.</p>"},{"location":"1_overview/#how-to-read-this-documentation","title":"How to Read This Documentation","text":"<p>The documentation is structured progressively:</p> <ol> <li>Overall system architecture.</li> <li>Performance and latency considerations.</li> <li>Key engineering decisions.</li> <li>Current limitations and technical debt.</li> <li>Production and operational considerations.</li> </ol> <p>Each section can be read independently, but the full value emerges from understanding the system as a whole.</p>"},{"location":"2_architecture/","title":"Architecture draft","text":"<ol> <li>architecture.md \u2014 Architecture &amp; Data Flow (muy importante)</li> </ol> <p>Ac\u00e1 demostr\u00e1s ingenier\u00eda, no marketing.</p> <p>Secciones internas:</p> <p>Pipeline decomposition</p> <p>Ingestion</p> <p>Pre-processing</p> <p>Inference</p> <p>Post-processing</p> <p>Output</p> <p>Process model</p> <p>Multiprocess architecture</p> <p>IPC via shared memory (NVMM)</p> <p>Why processes, not threads</p> <p>Backpressure handling</p> <p>Queues in GStreamer</p> <p>Trade-offs (no explicit drop policy yet)</p> <p>Nada de detalles implementativos finos. Nada de nombres de clases. Solo decisiones.</p>"},{"location":"3_performance/","title":"Performance draft","text":"<ol> <li>performance.md \u2014 Performance &amp; Measurements (clave)</li> </ol> <p>Esto es lo que m\u00e1s pesa para perfiles como el tuyo.</p> <p>Contenido:</p> <p>Throughput</p> <p>Streams, resolution, FPS</p> <p>Latency</p> <p>Explicar que es pipeline dividido</p> <p>D\u00f3nde se mide y por qu\u00e9</p> <p>Resource utilization</p> <p>GPU %</p> <p>VRAM</p> <p>CPU / RAM (si suma)</p> <p>What was improved</p> <p>Before: CPU-bound, not real-time</p> <p>After: real-time with quality trade-offs</p> <p>No pongas gr\u00e1ficos si no aportan. N\u00fameros claros &gt; gr\u00e1ficos lindos.</p>"},{"location":"4_engineering-decisions/","title":"Engineering Decisions draft","text":"<p>Ac\u00e1 se ve madurez.</p> <p>Ejemplos de secciones:</p> <p>Why GStreamer + DeepStream</p> <p>Why GPU-first design</p> <p>Why partial zero-copy</p> <p>Why Python (and where it hurts)</p> <p>En cada una:</p> <p>decisi\u00f3n</p> <p>beneficio</p> <p>costo / limitaci\u00f3n</p> <p>Eso es pensamiento de ingeniero, no de junior.</p>"},{"location":"5_limitations/","title":"Limitations draft","text":"<p>Esto no te debilita, te fortalece.</p> <p>Contenido:</p> <p>No full end-to-end zero-copy</p> <p>No horizontal scaling</p> <p>Limited hot-reload (no source swap)</p> <p>No advanced fault isolation yet</p> <p>Corto, honesto, t\u00e9cnico.</p>"},{"location":"6_production/","title":"Production draft","text":"<p>Solo si es real.</p> <p>24/7 operation assumptions</p> <p>Restart strategy</p> <p>Graceful shutdown</p> <p>Configuration hot-reload via DSL</p> <p>Nada de promesas futuras.</p>"},{"location":"","title":"Stream Inference Engine","text":"<p>Contenido m\u00ednimo, directo:</p> <p>Esto es lo primero que leen. Debe poder escanearse en 20\u201330 segundos.</p> <p>Contenido:</p> <p>One-liner</p> <p>Real-time multi-stream inference engine for computer vision workloads on GPU.</p> <p>Problem statement</p> <p>CPU-based pipelines couldn\u2019t meet real-time constraints.</p> <p>Target: multi-RTSP, low latency, production 24/7.</p> <p>High-level architecture</p> <p>5\u20136 bullets m\u00e1ximo.</p> <p>Sin c\u00f3digo.</p> <p>Sin nombres internos sensibles.</p> <p>Key results</p> <p>6 RTSP streams @ 720p / 25 FPS</p> <p>&lt;1% frame drop under load</p> <p>GPU-accelerated inference pipeline</p> <p>What this page is / is not</p> <p>Is: system design &amp; engineering decisions</p> <p>Is not: full implementation details or proprietary code</p> <p>Esto ya filtra bien a quien entiende y a quien no.</p>"},{"location":"es/2_architecture/","title":"2 architecture","text":"<ol> <li>architecture.md \u2014 Architecture &amp; Data Flow (muy importante)</li> </ol> <p>Ac\u00e1 demostr\u00e1s ingenier\u00eda, no marketing.</p> <p>Secciones internas:</p> <p>Pipeline decomposition</p> <p>Ingestion</p> <p>Pre-processing</p> <p>Inference</p> <p>Post-processing</p> <p>Output</p> <p>Process model</p> <p>Multiprocess architecture</p> <p>IPC via shared memory (NVMM)</p> <p>Why processes, not threads</p> <p>Backpressure handling</p> <p>Queues in GStreamer</p> <p>Trade-offs (no explicit drop policy yet)</p> <p>Nada de detalles implementativos finos. Nada de nombres de clases. Solo decisiones.</p>"},{"location":"es/3_performance/","title":"3 performance","text":"<ol> <li>performance.md \u2014 Performance &amp; Measurements (clave)</li> </ol> <p>Esto es lo que m\u00e1s pesa para perfiles como el tuyo.</p> <p>Contenido:</p> <p>Throughput</p> <p>Streams, resolution, FPS</p> <p>Latency</p> <p>Explicar que es pipeline dividido</p> <p>D\u00f3nde se mide y por qu\u00e9</p> <p>Resource utilization</p> <p>GPU %</p> <p>VRAM</p> <p>CPU / RAM (si suma)</p> <p>What was improved</p> <p>Before: CPU-bound, not real-time</p> <p>After: real-time with quality trade-offs</p> <p>No pongas gr\u00e1ficos si no aportan. N\u00fameros claros &gt; gr\u00e1ficos lindos.</p>"},{"location":"es/4_engineering-decisions/","title":"4 engineering decisions","text":"<p>Ac\u00e1 se ve madurez.</p> <p>Ejemplos de secciones:</p> <p>Why GStreamer + DeepStream</p> <p>Why GPU-first design</p> <p>Why partial zero-copy</p> <p>Why Python (and where it hurts)</p> <p>En cada una:</p> <p>decisi\u00f3n</p> <p>beneficio</p> <p>costo / limitaci\u00f3n</p> <p>Eso es pensamiento de ingeniero, no de junior.</p>"},{"location":"es/5_limitations/","title":"5 limitations","text":"<p>Esto no te debilita, te fortalece.</p> <p>Contenido:</p> <p>No full end-to-end zero-copy</p> <p>No horizontal scaling</p> <p>Limited hot-reload (no source swap)</p> <p>No advanced fault isolation yet</p> <p>Corto, honesto, t\u00e9cnico.</p>"},{"location":"es/6_production/","title":"6 production","text":"<p>Solo si es real.</p> <p>24/7 operation assumptions</p> <p>Restart strategy</p> <p>Graceful shutdown</p> <p>Configuration hot-reload via DSL</p> <p>Nada de promesas futuras.</p>"},{"location":"es/","title":"Motor de inferencia de Streams","text":"<p>Documentaci\u00f3n T\u00e9cnica</p>"}]}